{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq based Q&A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on bAbl tasks dataset https://research.fb.com/downloads/babi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download our dataset from http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz or a backup https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2c99c0187626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdwn_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# dwn_url = 'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdwn_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tasks-ds.tar.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/handson.ai/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/handson.ai/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/handson.ai/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/handson.ai/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 582\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/handson.ai/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/handson.ai/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/handson.ai/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "import urllib.request as urllib\n",
    "\n",
    "dwn_url = 'http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz'\n",
    "# dwn_url = 'https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz'\n",
    "urllib.urlretrieve(dwn_url, 'tasks-ds.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the tar \n",
      " tasksv11\n",
      "tasksv11/en\n",
      "tasksv11/._LICENSE\n",
      "tasksv11/LICENSE\n",
      "tasksv11/README\n",
      "tasksv11/shuffled\n",
      "tasksv11/shuffled/qa10_indefinite-knowledge_test.txt\n",
      "tasksv11/shuffled/qa10_indefinite-knowledge_train.txt\n",
      "tasksv11/shuffled/qa11_basic-coreference_test.txt\n",
      "tasksv11/shuffled/qa11_basic-coreference_train.txt\n",
      "tasksv11/shuffled/qa12_conjunction_test.txt\n",
      "tasksv11/shuffled/qa12_conjunction_train.txt\n",
      "tasksv11/shuffled/qa13_compound-coreference_test.txt\n",
      "tasksv11/shuffled/qa13_compound-coreference_train.txt\n",
      "tasksv11/shuffled/qa14_time-reasoning_test.txt\n",
      "tasksv11/shuffled/qa14_time-reasoning_train.txt\n",
      "tasksv11/shuffled/qa15_basic-deduction_test.txt\n",
      "tasksv11/shuffled/qa15_basic-deduction_train.txt\n",
      "tasksv11/shuffled/qa16_basic-induction_test.txt\n",
      "tasksv11/shuffled/qa16_basic-induction_train.txt\n",
      "tasksv11/shuffled/qa17_positional-reasoning_test.txt\n",
      "tasksv11/shuffled/qa17_positional-reasoning_train.txt\n",
      "tasksv11/shuffled/qa18_size-reasoning_test.txt\n",
      "tasksv11/shuffled/qa18_size-reasoning_train.txt\n",
      "tasksv11/shuffled/qa19_path-finding_test.txt\n",
      "tasksv11/shuffled/qa19_path-finding_train.txt\n",
      "tasksv11/shuffled/qa1_single-supporting-fact_test.txt\n",
      "tasksv11/shuffled/qa1_single-supporting-fact_train.txt\n",
      "tasksv11/shuffled/qa20_agents-motivations_test.txt\n",
      "tasksv11/shuffled/qa20_agents-motivations_train.txt\n",
      "tasksv11/shuffled/qa2_two-supporting-facts_test.txt\n",
      "tasksv11/shuffled/qa2_two-supporting-facts_train.txt\n",
      "tasksv11/shuffled/qa3_three-supporting-facts_test.txt\n",
      "tasksv11/shuffled/qa3_three-supporting-facts_train.txt\n",
      "tasksv11/shuffled/qa4_two-arg-relations_test.txt\n",
      "tasksv11/shuffled/qa4_two-arg-relations_train.txt\n",
      "tasksv11/shuffled/qa5_three-arg-relations_test.txt\n",
      "tasksv11/shuffled/qa5_three-arg-relations_train.txt\n",
      "tasksv11/shuffled/qa6_yes-no-questions_test.txt\n",
      "tasksv11/shuffled/qa6_yes-no-questions_train.txt\n",
      "tasksv11/shuffled/qa7_counting_test.txt\n",
      "tasksv11/shuffled/qa7_counting_train.txt\n",
      "tasksv11/shuffled/qa8_lists-sets_test.txt\n",
      "tasksv11/shuffled/qa8_lists-sets_train.txt\n",
      "tasksv11/shuffled/qa9_simple-negation_test.txt\n",
      "tasksv11/shuffled/qa9_simple-negation_train.txt\n",
      "tasksv11/en/qa10_indefinite-knowledge_test.txt\n",
      "tasksv11/en/qa10_indefinite-knowledge_train.txt\n",
      "tasksv11/en/qa11_basic-coreference_test.txt\n",
      "tasksv11/en/qa11_basic-coreference_train.txt\n",
      "tasksv11/en/qa12_conjunction_test.txt\n",
      "tasksv11/en/qa12_conjunction_train.txt\n",
      "tasksv11/en/qa13_compound-coreference_test.txt\n",
      "tasksv11/en/qa13_compound-coreference_train.txt\n",
      "tasksv11/en/qa14_time-reasoning_test.txt\n",
      "tasksv11/en/qa14_time-reasoning_train.txt\n",
      "tasksv11/en/qa15_basic-deduction_test.txt\n",
      "tasksv11/en/qa15_basic-deduction_train.txt\n",
      "tasksv11/en/qa16_basic-induction_test.txt\n",
      "tasksv11/en/qa16_basic-induction_train.txt\n",
      "tasksv11/en/qa17_positional-reasoning_test.txt\n",
      "tasksv11/en/qa17_positional-reasoning_train.txt\n",
      "tasksv11/en/qa18_size-reasoning_test.txt\n",
      "tasksv11/en/qa18_size-reasoning_train.txt\n",
      "tasksv11/en/qa19_path-finding_test.txt\n",
      "tasksv11/en/qa19_path-finding_train.txt\n",
      "tasksv11/en/qa1_single-supporting-fact_test.txt\n",
      "tasksv11/en/qa1_single-supporting-fact_train.txt\n",
      "tasksv11/en/qa20_agents-motivations_test.txt\n",
      "tasksv11/en/qa20_agents-motivations_train.txt\n",
      "tasksv11/en/qa2_two-supporting-facts_test.txt\n",
      "tasksv11/en/qa2_two-supporting-facts_train.txt\n",
      "tasksv11/en/qa3_three-supporting-facts_test.txt\n",
      "tasksv11/en/qa3_three-supporting-facts_train.txt\n",
      "tasksv11/en/qa4_two-arg-relations_test.txt\n",
      "tasksv11/en/qa4_two-arg-relations_train.txt\n",
      "tasksv11/en/qa5_three-arg-relations_test.txt\n",
      "tasksv11/en/qa5_three-arg-relations_train.txt\n",
      "tasksv11/en/qa6_yes-no-questions_test.txt\n",
      "tasksv11/en/qa6_yes-no-questions_train.txt\n",
      "tasksv11/en/qa7_counting_test.txt\n",
      "tasksv11/en/qa7_counting_train.txt\n",
      "tasksv11/en/qa8_lists-sets_test.txt\n",
      "tasksv11/en/qa8_lists-sets_train.txt\n",
      "tasksv11/en/qa9_simple-negation_test.txt\n",
      "tasksv11/en/qa9_simple-negation_train.txt\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "tar = tarfile.open('tasks-ds.tar.gz')\n",
    "print('Files in the tar \\n', '\\n'.join(tar.getnames()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facebook [bAbI](https://research.fb.com/downloads/babi/) project dataset has next format.\n",
    "\n",
    "```\n",
    "ID text\n",
    "ID question[tab]answer[tab]supporting fact IDS.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''tokenize inlcuding punctuation'''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        s_id, line = line.split(' ', 1)\n",
    "        s_id = int(s_id)\n",
    "\n",
    "        if s_id == 1:\n",
    "            story = []\n",
    "\n",
    "        if '\\t' in line:\n",
    "            q, a, sup = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "\n",
    "            # Provide all the substories\n",
    "            substory = [x for x in story if x]\n",
    "\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            story.append(tokenize(line))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def get_stories(f):\n",
    "    '''read lines from file and convert sentences into a single story.'''\n",
    "    lines = f.readlines()\n",
    "    data = parse_stories(lines)\n",
    "\n",
    "    def flatten(data): return reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_extract = 'tasksv11/en/qa1_single-supporting-fact_{}.txt'\n",
    "\n",
    "test = tar.extractfile(file_to_extract.format('test'))\n",
    "train = tar.extractfile(file_to_extract.format('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/handson.ai/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "test_data = get_stories(test)\n",
    "train_data = get_stories(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Mary',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hallway',\n",
       "  '.',\n",
       "  'Daniel',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'moved',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'journeyed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hallway',\n",
       "  '.',\n",
       "  'Mary',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bedroom',\n",
       "  '.',\n",
       "  'Mary',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'garden',\n",
       "  '.',\n",
       "  'Sandra',\n",
       "  'went',\n",
       "  'back',\n",
       "  'to',\n",
       "  'the',\n",
       "  'bathroom',\n",
       "  '.',\n",
       "  'John',\n",
       "  'went',\n",
       "  'to',\n",
       "  'the',\n",
       "  'office',\n",
       "  '.'],\n",
       " ['Where', 'is', 'John', '?'],\n",
       " 'office')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.choice(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set('#')\n",
    "\n",
    "for story, q, answer in train_data + test_data:\n",
    "    vocab |= set(story + q + [answer])\n",
    "\n",
    "vocab = sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " '.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'Where',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'garden',\n",
       " 'hallway',\n",
       " 'is',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'moved',\n",
       " 'office',\n",
       " 'the',\n",
       " 'to',\n",
       " 'travelled',\n",
       " 'went']"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_data + test_data)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_data + test_data)))\n",
    "answer_maxlen = max(map(len, (x for _, _, x in train_data + test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 22 unique words\n",
      "Story max length: 66 words\n",
      "Query max length: 4 words\n",
      "Answer max length: 8 words\n",
      "Number of training stories: 1000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['John', 'went', 'to', 'the', 'bathroom', '.', 'John', 'moved', 'to', 'the', 'office', '.', 'John', 'went', 'to', 'the', 'hallway', '.', 'Sandra', 'journeyed', 'to', 'the', 'hallway', '.', 'John', 'journeyed', 'to', 'the', 'bathroom', '.', 'Daniel', 'went', 'to', 'the', 'bathroom', '.', 'Sandra', 'moved', 'to', 'the', 'garden', '.', 'John', 'moved', 'to', 'the', 'office', '.', 'Sandra', 'moved', 'to', 'the', 'hallway', '.', 'Daniel', 'went', 'to', 'the', 'kitchen', '.'], ['Where', 'is', 'Daniel', '?'], 'kitchen')\n"
     ]
    }
   ],
   "source": [
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Answer max length:', answer_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_data))\n",
    "print('Number of test stories:', len(test_data))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(random.choice(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing the word sequences...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing the word sequences...')\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = [word_idx[answer]]\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (X, Xq, Y)\n",
    "\n",
    "\n",
    "# create words index\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "# vrctorize input\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(\n",
    "    train_data, word_idx, story_maxlen, query_maxlen)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(\n",
    "    test_data, word_idx, story_maxlen, query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 19, 18, 17, 1, 6, 20, 19, 18, 9, 1, 6, 20, 19, 18, 17, 1, 5, 21, 8, 19, 18, 12, 1, 3, 21, 8, 19, 18, 17, 1, 6, 21, 19, 18, 9, 1, 4, 14, 19, 18, 10, 1, 6, 21, 8, 19, 18, 17, 1, 3, 21, 19, 18, 10, 1, 6, 20, 19, 18, 11, 1]\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(inputs_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![End-To-End Memory Networks](http://www.zmonster.me/assets/img/memn2n_single_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # define hyperparameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # define architecture\n",
    "        self.embedding = nn.Embedding(22, hidden_size)\n",
    "#         self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, input_sequence, hidden_state):\n",
    "        seq_len = len(input_sequence)\n",
    "\n",
    "        embedded = F.dropout(self.embedding(input_sequence.unsqueeze(0)), 0.3)\n",
    "#         output, hidden = self.gru(embedded, hidden_state)\n",
    "        return embedded  # output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # define hyperparameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # define architecture\n",
    "        self.fc1 = nn.Linear(input_size * hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, answer):\n",
    "        answer = answer.view(-1)\n",
    "        answer = F.dropout(self.fc1(answer), 0.5)\n",
    "        answer = F.dropout(self.fc2(answer), 0.5)\n",
    "        answer = self.fc3(answer)\n",
    "        answer = F.softmax(answer, dim=0)\n",
    "\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Memory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(Memory, self).__init__()\n",
    "\n",
    "        # define hyperparameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # define architecture\n",
    "        self.m_encoder = Encoder(input_size, hidden_size, n_layers)\n",
    "        self.c_encoder = Encoder(input_size, hidden_size, n_layers)\n",
    "        self.question_encoder = Encoder(input_size, hidden_size, n_layers)\n",
    "\n",
    "        self.memory = nn.GRU(hidden_size * 2, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, fact, query, hidden_state):\n",
    "        encoded_m = self.m_encoder(fact, hidden_state)\n",
    "        encoded_question = self.question_encoder(query, hidden_state)\n",
    "\n",
    "        match = torch.dot(encoded_m, encoded_question)\n",
    "#         print(encoded_m.size(), encoded_question.size(), match)\n",
    "        match = F.softmax(match, dim=0)\n",
    "\n",
    "        encoded_c = self.c_encoder(fact, hidden_state)\n",
    "        response = torch.add(match, 1, encoded_c)\n",
    "\n",
    "        answer = torch.cat((response, encoded_question))\n",
    "        answer = answer.unsqueeze(0).view(fact.size()[0], 1, -1)\n",
    "\n",
    "        answer, hidden_state = self.memory(answer, hidden_state)\n",
    "\n",
    "        return answer, hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact 49 question 132\n",
      "expected answer: [ kitchen ], predicted answer: word idx 1 word: [ . ] torch.Size([22])\n"
     ]
    }
   ],
   "source": [
    "def pad(array, max_len):\n",
    "    return np.pad(array, (0, max_len - len(array) % max_len), 'constant')\n",
    "\n",
    "\n",
    "m_input_size = 132\n",
    "m_hidden_size = 5\n",
    "m_n_layers = 1\n",
    "\n",
    "case_idx = random.randrange(0, len(train_data))\n",
    "\n",
    "fact = pad(inputs_train[case_idx], m_input_size)\n",
    "question = pad(queries_train[case_idx], m_input_size)\n",
    "answer = answers_train[case_idx]\n",
    "\n",
    "print('fact', len(inputs_train[3]), 'question', len(question))\n",
    "\n",
    "memory = Memory(m_input_size, m_hidden_size, m_n_layers)\n",
    "\n",
    "m_hidden_state = Variable(torch.zeros(m_n_layers, 1, m_hidden_size))\n",
    "\n",
    "a_output_size = vocab_size\n",
    "\n",
    "fact = Variable(torch.LongTensor(fact))\n",
    "question = Variable(torch.LongTensor(question))\n",
    "\n",
    "prediction_for_answer, hidden_state = memory(fact, question, m_hidden_state)\n",
    "\n",
    "decoder = Decoder(m_input_size, m_hidden_size, a_output_size, m_n_layers)\n",
    "\n",
    "predicted_answer = decoder(prediction_for_answer)\n",
    "_, word_idx = torch.max(predicted_answer.data, 0)\n",
    "print('expected answer: [', vocab[answer[0]], '], predicted answer:',\n",
    "      'word idx', word_idx[0], 'word: [', vocab[word_idx[0]], ']', predicted_answer.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 0.5\n",
    "\n",
    "\n",
    "def train(fact, question, answer, hidden_state, memory_model, answer_model, memory_optimizer, answer_optimizer, criterion):\n",
    "\n",
    "    # reset gradients\n",
    "    memory_optimizer.zero_grad()\n",
    "    answer_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    # orchestrate model learning\n",
    "    answer_support, hidden_state = memory_model(fact, question, hidden_state)\n",
    "    predicted_answer = answer_model(answer_support)\n",
    "\n",
    "    _, word_idx = torch.max(predicted_answer.data, 0)\n",
    "#     print('predicted asnwer',\n",
    "#           word_idx[0], 'excpected answer', answer.data[0], predicted_answer.size())\n",
    "\n",
    "    # compute loss\n",
    "    predicted_answer = predicted_answer.view(1, -1)\n",
    "    loss = criterion(predicted_answer, answer)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(memory_model.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(answer_model.parameters(), clip)\n",
    "    memory_optimizer.step()\n",
    "    answer_optimizer.step()\n",
    "\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model parameters\n",
    "tr_input_size = 132\n",
    "tr_hidden_size = 5\n",
    "tr_n_layers = 1\n",
    "tr_output_size = vocab_size\n",
    "\n",
    "# init model\n",
    "memory_model = Memory(tr_input_size, tr_hidden_size, tr_n_layers)\n",
    "answer_model = Decoder(tr_input_size, tr_hidden_size,\n",
    "                       tr_output_size, tr_n_layers)\n",
    "\n",
    "# init optimizers and loss calculator\n",
    "lr = 0.0001\n",
    "memory_optimizer = optim.RMSprop(memory_model.parameters(), lr=lr)\n",
    "answer_optimizer = optim.RMSprop(answer_model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through\n",
    "n_epochs = 2000\n",
    "n_iterations = len(train_data)\n",
    "trained_model_path = 'qna-at-epoch-{}.pt'\n",
    "\n",
    "# keep track of time elapsed and running averages\n",
    "plot_every = 1000\n",
    "print_every = 500\n",
    "save_model_every = 1  # save model every epoch\n",
    "\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # reset every print_every\n",
    "plot_loss_total = 0  # reset every plot_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is about to start training...\n",
      "0m 17s (- 1197m 40s) (s 500 in e 1 0%) 3.0572\n",
      "0m 54s (- 1201m 38s) (s 1499 in e 2 0%) 5.9910\n",
      "1m 29s (- 1187m 45s) (s 2498 in e 3 0%) 5.9596\n",
      "2m 4s (- 1183m 3s) (s 3497 in e 4 0%) 5.9483\n",
      "2m 39s (- 1176m 33s) (s 4496 in e 5 0%) 5.9447\n",
      "3m 13s (- 1170m 43s) (s 5495 in e 6 0%) 5.9440\n",
      "3m 48s (- 1168m 43s) (s 6494 in e 7 0%) 5.9443\n",
      "4m 23s (- 1167m 46s) (s 7493 in e 8 0%) 5.9452\n",
      "4m 58s (- 1167m 16s) (s 8492 in e 9 0%) 5.9445\n",
      "5m 33s (- 1165m 38s) (s 9491 in e 10 0%) 5.9425\n",
      "6m 8s (- 1163m 24s) (s 10490 in e 11 0%) 5.9424\n",
      "6m 42s (- 1161m 6s) (s 11489 in e 12 0%) 5.9421\n",
      "7m 17s (- 1159m 55s) (s 12488 in e 13 0%) 5.9371\n",
      "8m 3s (- 1186m 55s) (s 13487 in e 14 0%) 5.9384\n",
      "8m 39s (- 1185m 42s) (s 14486 in e 15 0%) 5.9388\n",
      "9m 13s (- 1182m 48s) (s 15485 in e 16 0%) 5.9355\n",
      "9m 48s (- 1179m 39s) (s 16484 in e 17 0%) 5.9364\n",
      "10m 22s (- 1177m 7s) (s 17483 in e 18 0%) 5.9327\n",
      "10m 57s (- 1174m 35s) (s 18482 in e 19 0%) 5.9371\n",
      "11m 31s (- 1172m 13s) (s 19481 in e 20 0%) 5.9343\n",
      "12m 6s (- 1170m 1s) (s 20480 in e 21 1%) 5.9347\n",
      "12m 41s (- 1168m 49s) (s 21479 in e 22 1%) 5.9339\n",
      "13m 20s (- 1173m 14s) (s 22478 in e 23 1%) 5.9350\n",
      "14m 6s (- 1187m 21s) (s 23477 in e 24 1%) 5.9301\n",
      "14m 51s (- 1199m 33s) (s 24476 in e 25 1%) 5.9321\n",
      "15m 37s (- 1210m 47s) (s 25475 in e 26 1%) 5.9315\n",
      "16m 22s (- 1221m 10s) (s 26474 in e 27 1%) 5.9330\n",
      "17m 8s (- 1231m 4s) (s 27473 in e 28 1%) 5.9279\n",
      "17m 54s (- 1239m 39s) (s 28472 in e 29 1%) 5.9254\n",
      "18m 29s (- 1236m 30s) (s 29471 in e 30 1%) 5.9267\n",
      "19m 3s (- 1231m 25s) (s 30470 in e 31 1%) 5.9296\n",
      "19m 36s (- 1226m 51s) (s 31469 in e 32 1%) 5.9215\n",
      "20m 10s (- 1222m 23s) (s 32468 in e 33 1%) 5.9251\n",
      "20m 43s (- 1218m 9s) (s 33467 in e 34 1%) 5.9225\n",
      "21m 17s (- 1214m 12s) (s 34466 in e 35 1%) 5.9209\n",
      "21m 50s (- 1210m 16s) (s 35465 in e 36 1%) 5.9189\n",
      "22m 24s (- 1206m 38s) (s 36464 in e 37 1%) 5.9106\n",
      "22m 57s (- 1203m 4s) (s 37463 in e 38 1%) 5.9172\n",
      "23m 31s (- 1199m 42s) (s 38462 in e 39 1%) 5.9127\n",
      "24m 4s (- 1196m 27s) (s 39461 in e 40 1%) 5.9079\n",
      "24m 38s (- 1193m 21s) (s 40460 in e 41 2%) 5.9120\n",
      "25m 12s (- 1190m 31s) (s 41459 in e 42 2%) 5.9128\n",
      "25m 45s (- 1187m 38s) (s 42458 in e 43 2%) 5.8959\n",
      "26m 19s (- 1184m 54s) (s 43457 in e 44 2%) 5.9011\n",
      "26m 52s (- 1182m 15s) (s 44456 in e 45 2%) 5.8909\n",
      "27m 26s (- 1179m 43s) (s 45455 in e 46 2%) 5.8954\n",
      "27m 59s (- 1177m 15s) (s 46454 in e 47 2%) 5.8904\n",
      "28m 33s (- 1174m 58s) (s 47453 in e 48 2%) 5.8890\n",
      "29m 6s (- 1172m 39s) (s 48452 in e 49 2%) 5.8810\n",
      "29m 40s (- 1170m 32s) (s 49451 in e 50 2%) 5.8767\n",
      "30m 14s (- 1168m 23s) (s 50450 in e 51 2%) 5.8682\n",
      "30m 47s (- 1166m 13s) (s 51449 in e 52 2%) 5.8571\n",
      "31m 21s (- 1164m 32s) (s 52448 in e 53 2%) 5.8504\n",
      "31m 55s (- 1162m 32s) (s 53447 in e 54 2%) 5.8392\n",
      "32m 28s (- 1160m 32s) (s 54446 in e 55 2%) 5.8482\n",
      "33m 2s (- 1158m 41s) (s 55445 in e 56 2%) 5.8304\n",
      "33m 35s (- 1156m 49s) (s 56444 in e 57 2%) 5.8065\n",
      "34m 9s (- 1154m 59s) (s 57443 in e 58 2%) 5.8177\n",
      "34m 43s (- 1153m 50s) (s 58442 in e 59 2%) 5.7954\n",
      "35m 24s (- 1155m 54s) (s 59441 in e 60 2%) 5.7975\n",
      "36m 11s (- 1161m 12s) (s 60440 in e 61 3%) 5.7998\n",
      "36m 58s (- 1166m 28s) (s 61439 in e 62 3%) 5.7823\n",
      "37m 44s (- 1171m 19s) (s 62438 in e 63 3%) 5.7675\n",
      "38m 31s (- 1175m 58s) (s 63437 in e 64 3%) 5.7595\n",
      "39m 18s (- 1180m 32s) (s 64436 in e 65 3%) 5.7627\n",
      "40m 4s (- 1184m 51s) (s 65435 in e 66 3%) 5.7512\n",
      "40m 51s (- 1189m 3s) (s 66434 in e 67 3%) 5.7404\n",
      "41m 34s (- 1191m 20s) (s 67433 in e 68 3%) 5.7459\n",
      "42m 8s (- 1189m 43s) (s 68432 in e 69 3%) 5.7470\n",
      "42m 43s (- 1187m 51s) (s 69431 in e 70 3%) 5.7244\n",
      "43m 16s (- 1185m 49s) (s 70430 in e 71 3%) 5.7445\n",
      "43m 50s (- 1183m 42s) (s 71429 in e 72 3%) 5.7266\n",
      "44m 23s (- 1181m 36s) (s 72428 in e 73 3%) 5.7148\n",
      "44m 57s (- 1179m 37s) (s 73427 in e 74 3%) 5.7026\n",
      "45m 30s (- 1177m 36s) (s 74426 in e 75 3%) 5.7319\n",
      "46m 4s (- 1175m 41s) (s 75425 in e 76 3%) 5.7022\n",
      "46m 38s (- 1173m 47s) (s 76424 in e 77 3%) 5.7198\n",
      "47m 11s (- 1171m 54s) (s 77423 in e 78 3%) 5.6739\n",
      "47m 45s (- 1170m 4s) (s 78422 in e 79 3%) 5.6761\n",
      "48m 18s (- 1168m 16s) (s 79421 in e 80 3%) 5.6627\n",
      "48m 52s (- 1166m 28s) (s 80420 in e 81 4%) 5.6811\n",
      "49m 26s (- 1164m 59s) (s 81419 in e 82 4%) 5.6599\n",
      "49m 59s (- 1163m 18s) (s 82418 in e 83 4%) 5.6632\n",
      "50m 33s (- 1161m 34s) (s 83417 in e 84 4%) 5.6585\n",
      "51m 6s (- 1159m 55s) (s 84416 in e 85 4%) 5.6315\n",
      "51m 40s (- 1158m 17s) (s 85415 in e 86 4%) 5.6476\n",
      "52m 13s (- 1156m 39s) (s 86414 in e 87 4%) 5.6431\n",
      "52m 47s (- 1155m 6s) (s 87413 in e 88 4%) 5.6265\n",
      "53m 21s (- 1153m 36s) (s 88412 in e 89 4%) 5.6438\n",
      "53m 54s (- 1152m 4s) (s 89411 in e 90 4%) 5.5869\n",
      "54m 28s (- 1150m 32s) (s 90410 in e 91 4%) 5.6082\n",
      "55m 2s (- 1149m 6s) (s 91409 in e 92 4%) 5.5933\n",
      "55m 35s (- 1147m 36s) (s 92408 in e 93 4%) 5.6122\n",
      "56m 9s (- 1146m 9s) (s 93407 in e 94 4%) 5.5805\n",
      "56m 42s (- 1144m 43s) (s 94406 in e 95 4%) 5.5729\n",
      "57m 16s (- 1143m 17s) (s 95405 in e 96 4%) 5.5141\n",
      "57m 49s (- 1141m 55s) (s 96404 in e 97 4%) 5.5823\n",
      "58m 23s (- 1140m 33s) (s 97403 in e 98 4%) 5.5484\n",
      "58m 56s (- 1139m 11s) (s 98402 in e 99 4%) 5.5389\n",
      "59m 30s (- 1137m 51s) (s 99401 in e 100 4%) 5.5182\n",
      "60m 4s (- 1136m 33s) (s 100400 in e 101 5%) 5.5633\n",
      "60m 37s (- 1135m 13s) (s 101399 in e 102 5%) 5.5471\n",
      "61m 11s (- 1133m 54s) (s 102398 in e 103 5%) 5.4969\n",
      "61m 44s (- 1132m 37s) (s 103397 in e 104 5%) 5.5365\n",
      "62m 18s (- 1131m 20s) (s 104396 in e 105 5%) 5.4896\n",
      "62m 52s (- 1130m 10s) (s 105395 in e 106 5%) 5.4758\n",
      "63m 25s (- 1128m 55s) (s 106394 in e 107 5%) 5.4663\n",
      "63m 59s (- 1127m 41s) (s 107393 in e 108 5%) 5.4448\n",
      "64m 32s (- 1126m 26s) (s 108392 in e 109 5%) 5.4462\n",
      "65m 6s (- 1125m 15s) (s 109391 in e 110 5%) 5.4970\n",
      "65m 39s (- 1124m 3s) (s 110390 in e 111 5%) 5.4646\n",
      "66m 13s (- 1122m 52s) (s 111389 in e 112 5%) 5.4528\n",
      "66m 46s (- 1121m 39s) (s 112388 in e 113 5%) 5.3840\n",
      "67m 20s (- 1120m 29s) (s 113387 in e 114 5%) 5.4112\n",
      "67m 54s (- 1119m 18s) (s 114386 in e 115 5%) 5.4435\n",
      "68m 27s (- 1118m 10s) (s 115385 in e 116 5%) 5.3775\n",
      "69m 1s (- 1117m 0s) (s 116384 in e 117 5%) 5.3866\n",
      "69m 34s (- 1115m 53s) (s 117383 in e 118 5%) 5.3222\n",
      "70m 8s (- 1114m 45s) (s 118382 in e 119 5%) 5.3148\n",
      "70m 42s (- 1113m 47s) (s 119381 in e 120 5%) 5.3513\n",
      "71m 15s (- 1112m 40s) (s 120380 in e 121 6%) 5.3595\n",
      "71m 49s (- 1111m 34s) (s 121379 in e 122 6%) 5.3130\n",
      "72m 22s (- 1110m 29s) (s 122378 in e 123 6%) 5.2583\n",
      "72m 56s (- 1109m 23s) (s 123377 in e 124 6%) 5.2834\n",
      "73m 29s (- 1108m 20s) (s 124376 in e 125 6%) 5.2725\n",
      "74m 3s (- 1107m 16s) (s 125375 in e 126 6%) 5.3048\n",
      "74m 36s (- 1106m 14s) (s 126374 in e 127 6%) 5.2655\n",
      "75m 10s (- 1105m 13s) (s 127373 in e 128 6%) 5.2930\n",
      "75m 44s (- 1104m 11s) (s 128372 in e 129 6%) 5.2746\n",
      "76m 17s (- 1103m 8s) (s 129371 in e 130 6%) 5.2688\n",
      "76m 51s (- 1102m 6s) (s 130370 in e 131 6%) 5.2658\n",
      "77m 24s (- 1101m 7s) (s 131369 in e 132 6%) 5.2511\n",
      "77m 58s (- 1100m 7s) (s 132368 in e 133 6%) 5.2266\n",
      "78m 31s (- 1099m 8s) (s 133367 in e 134 6%) 5.2070\n",
      "79m 5s (- 1098m 8s) (s 134366 in e 135 6%) 5.2728\n",
      "79m 38s (- 1097m 9s) (s 135365 in e 136 6%) 5.2400\n",
      "80m 12s (- 1096m 10s) (s 136364 in e 137 6%) 5.1888\n",
      "80m 45s (- 1095m 11s) (s 137363 in e 138 6%) 5.2182\n",
      "81m 19s (- 1094m 12s) (s 138362 in e 139 6%) 5.2189\n",
      "81m 52s (- 1093m 13s) (s 139361 in e 140 6%) 5.2294\n",
      "82m 26s (- 1092m 17s) (s 140360 in e 141 7%) 5.1991\n",
      "83m 0s (- 1091m 19s) (s 141359 in e 142 7%) 5.2419\n",
      "83m 34s (- 1090m 31s) (s 142358 in e 143 7%) 5.1874\n",
      "84m 8s (- 1089m 47s) (s 143357 in e 144 7%) 5.1422\n",
      "84m 42s (- 1088m 51s) (s 144356 in e 145 7%) 5.2130\n",
      "85m 15s (- 1087m 55s) (s 145355 in e 146 7%) 5.1690\n",
      "85m 49s (- 1086m 59s) (s 146354 in e 147 7%) 5.1840\n",
      "86m 22s (- 1086m 4s) (s 147353 in e 148 7%) 5.2446\n",
      "86m 56s (- 1085m 8s) (s 148352 in e 149 7%) 5.1896\n",
      "87m 29s (- 1084m 13s) (s 149351 in e 150 7%) 5.1774\n",
      "88m 3s (- 1083m 18s) (s 150350 in e 151 7%) 5.2297\n",
      "88m 37s (- 1082m 27s) (s 151349 in e 152 7%) 5.1391\n",
      "89m 10s (- 1081m 34s) (s 152348 in e 153 7%) 5.1618\n",
      "89m 44s (- 1080m 40s) (s 153347 in e 154 7%) 5.1888\n",
      "90m 17s (- 1079m 45s) (s 154346 in e 155 7%) 5.1811\n",
      "90m 51s (- 1078m 52s) (s 155345 in e 156 7%) 5.1805\n",
      "91m 24s (- 1077m 59s) (s 156344 in e 157 7%) 5.1469\n",
      "91m 58s (- 1077m 4s) (s 157343 in e 158 7%) 5.1729\n",
      "92m 31s (- 1076m 12s) (s 158342 in e 159 7%) 5.1739\n",
      "93m 5s (- 1075m 19s) (s 159341 in e 160 7%) 5.1291\n",
      "93m 38s (- 1074m 28s) (s 160340 in e 161 8%) 5.2036\n",
      "94m 12s (- 1073m 35s) (s 161339 in e 162 8%) 5.1618\n",
      "94m 45s (- 1072m 44s) (s 162338 in e 163 8%) 5.1311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95m 19s (- 1071m 53s) (s 163337 in e 164 8%) 5.1429\n",
      "95m 52s (- 1071m 1s) (s 164336 in e 165 8%) 5.1049\n",
      "96m 26s (- 1070m 10s) (s 165335 in e 166 8%) 5.1004\n",
      "97m 0s (- 1069m 20s) (s 166334 in e 167 8%) 5.1177\n",
      "97m 33s (- 1068m 29s) (s 167333 in e 168 8%) 5.1462\n",
      "98m 7s (- 1067m 38s) (s 168332 in e 169 8%) 5.1303\n",
      "98m 40s (- 1066m 47s) (s 169331 in e 170 8%) 5.1426\n",
      "99m 14s (- 1065m 57s) (s 170330 in e 171 8%) 5.1457\n",
      "99m 47s (- 1065m 7s) (s 171329 in e 172 8%) 5.1369\n",
      "100m 20s (- 1064m 17s) (s 172328 in e 173 8%) 5.1642\n",
      "100m 54s (- 1063m 28s) (s 173327 in e 174 8%) 5.0710\n",
      "101m 28s (- 1062m 43s) (s 174326 in e 175 8%) 5.1740\n",
      "102m 1s (- 1061m 53s) (s 175325 in e 176 8%) 5.1576\n",
      "102m 35s (- 1061m 4s) (s 176324 in e 177 8%) 5.1402\n",
      "103m 9s (- 1060m 16s) (s 177323 in e 178 8%) 5.1326\n",
      "103m 42s (- 1059m 27s) (s 178322 in e 179 8%) 5.1231\n",
      "104m 16s (- 1058m 38s) (s 179321 in e 180 8%) 5.1035\n",
      "104m 49s (- 1057m 49s) (s 180320 in e 181 9%) 5.1084\n",
      "105m 23s (- 1057m 1s) (s 181319 in e 182 9%) 5.1450\n",
      "105m 56s (- 1056m 12s) (s 182318 in e 183 9%) 5.1032\n",
      "106m 29s (- 1055m 24s) (s 183317 in e 184 9%) 5.1361\n",
      "107m 3s (- 1054m 36s) (s 184316 in e 185 9%) 5.0639\n",
      "107m 36s (- 1053m 47s) (s 185315 in e 186 9%) 5.1266\n",
      "108m 10s (- 1052m 59s) (s 186314 in e 187 9%) 5.1156\n",
      "108m 43s (- 1052m 11s) (s 187313 in e 188 9%) 5.1048\n",
      "109m 17s (- 1051m 23s) (s 188312 in e 189 9%) 5.0865\n",
      "109m 50s (- 1050m 36s) (s 189311 in e 190 9%) 5.1087\n",
      "110m 24s (- 1049m 49s) (s 190310 in e 191 9%) 5.0849\n",
      "110m 57s (- 1049m 2s) (s 191309 in e 192 9%) 5.0735\n",
      "111m 31s (- 1048m 17s) (s 192308 in e 193 9%) 5.0683\n",
      "112m 4s (- 1047m 30s) (s 193307 in e 194 9%) 5.0946\n",
      "112m 38s (- 1046m 43s) (s 194306 in e 195 9%) 5.1162\n",
      "113m 11s (- 1045m 57s) (s 195305 in e 196 9%) 5.1202\n",
      "113m 45s (- 1045m 10s) (s 196304 in e 197 9%) 5.0774\n",
      "114m 18s (- 1044m 23s) (s 197303 in e 198 9%) 5.0992\n",
      "114m 52s (- 1043m 38s) (s 198302 in e 199 9%) 5.0929\n",
      "115m 25s (- 1042m 51s) (s 199301 in e 200 9%) 5.1085\n",
      "115m 58s (- 1042m 4s) (s 200300 in e 201 10%) 5.0628\n",
      "116m 32s (- 1041m 18s) (s 201299 in e 202 10%) 5.0240\n",
      "117m 5s (- 1040m 32s) (s 202298 in e 203 10%) 5.0953\n",
      "117m 39s (- 1039m 48s) (s 203297 in e 204 10%) 5.0674\n",
      "118m 12s (- 1039m 3s) (s 204296 in e 205 10%) 5.0622\n",
      "118m 46s (- 1038m 18s) (s 205295 in e 206 10%) 5.0774\n",
      "119m 19s (- 1037m 34s) (s 206294 in e 207 10%) 5.0993\n",
      "119m 54s (- 1036m 55s) (s 207293 in e 208 10%) 5.0734\n",
      "120m 27s (- 1036m 10s) (s 208292 in e 209 10%) 5.0680\n",
      "121m 1s (- 1035m 25s) (s 209291 in e 210 10%) 5.0427\n",
      "121m 34s (- 1034m 40s) (s 210290 in e 211 10%) 5.0621\n",
      "122m 7s (- 1033m 55s) (s 211289 in e 212 10%) 5.0218\n",
      "122m 41s (- 1033m 11s) (s 212288 in e 213 10%) 5.0800\n",
      "123m 15s (- 1032m 28s) (s 213287 in e 214 10%) 5.0175\n",
      "123m 48s (- 1031m 44s) (s 214286 in e 215 10%) 5.0812\n",
      "124m 22s (- 1031m 0s) (s 215285 in e 216 10%) 5.0469\n",
      "124m 55s (- 1030m 17s) (s 216284 in e 217 10%) 5.0266\n",
      "125m 29s (- 1029m 33s) (s 217283 in e 218 10%) 5.0476\n",
      "126m 2s (- 1028m 49s) (s 218282 in e 219 10%) 5.0569\n",
      "126m 36s (- 1028m 6s) (s 219281 in e 220 10%) 5.0668\n",
      "127m 9s (- 1027m 22s) (s 220280 in e 221 11%) 5.0379\n",
      "127m 43s (- 1026m 38s) (s 221279 in e 222 11%) 5.0565\n",
      "128m 16s (- 1025m 56s) (s 222278 in e 223 11%) 5.0313\n",
      "128m 50s (- 1025m 12s) (s 223277 in e 224 11%) 5.0715\n",
      "129m 23s (- 1024m 29s) (s 224276 in e 225 11%) 5.0084\n",
      "129m 57s (- 1023m 46s) (s 225275 in e 226 11%) 5.0406\n",
      "130m 30s (- 1023m 3s) (s 226274 in e 227 11%) 5.0691\n",
      "131m 4s (- 1022m 20s) (s 227273 in e 228 11%) 5.0480\n",
      "131m 37s (- 1021m 37s) (s 228272 in e 229 11%) 5.0707\n",
      "132m 11s (- 1020m 54s) (s 229271 in e 230 11%) 5.0182\n",
      "132m 44s (- 1020m 11s) (s 230270 in e 231 11%) 5.0333\n",
      "133m 18s (- 1019m 30s) (s 231269 in e 232 11%) 5.0476\n",
      "133m 51s (- 1018m 47s) (s 232268 in e 233 11%) 5.0190\n",
      "134m 25s (- 1018m 5s) (s 233267 in e 234 11%) 5.0474\n",
      "134m 58s (- 1017m 23s) (s 234266 in e 235 11%) 5.0421\n",
      "135m 32s (- 1016m 40s) (s 235265 in e 236 11%) 5.0147\n",
      "136m 5s (- 1015m 58s) (s 236264 in e 237 11%) 5.0291\n",
      "136m 39s (- 1015m 16s) (s 237263 in e 238 11%) 5.0519\n",
      "137m 12s (- 1014m 35s) (s 238262 in e 239 11%) 5.0106\n",
      "137m 46s (- 1013m 53s) (s 239261 in e 240 11%) 5.1042\n",
      "138m 19s (- 1013m 11s) (s 240260 in e 241 12%) 5.0423\n",
      "138m 53s (- 1012m 29s) (s 241259 in e 242 12%) 5.0394\n",
      "139m 27s (- 1011m 48s) (s 242258 in e 243 12%) 5.0592\n",
      "140m 0s (- 1011m 6s) (s 243257 in e 244 12%) 5.0290\n",
      "140m 34s (- 1010m 25s) (s 244256 in e 245 12%) 4.9987\n",
      "141m 7s (- 1009m 43s) (s 245255 in e 246 12%) 5.0352\n",
      "141m 41s (- 1009m 1s) (s 246254 in e 247 12%) 4.9968\n",
      "142m 14s (- 1008m 20s) (s 247253 in e 248 12%) 5.0108\n",
      "142m 48s (- 1007m 41s) (s 248252 in e 249 12%) 5.0679\n",
      "143m 22s (- 1007m 0s) (s 249251 in e 250 12%) 5.0402\n",
      "143m 57s (- 1006m 30s) (s 250250 in e 251 12%) 5.0063\n",
      "144m 30s (- 1005m 48s) (s 251249 in e 252 12%) 5.0352\n",
      "145m 4s (- 1005m 9s) (s 252248 in e 253 12%) 5.0055\n",
      "145m 37s (- 1004m 28s) (s 253247 in e 254 12%) 5.0194\n",
      "146m 11s (- 1003m 47s) (s 254246 in e 255 12%) 5.0209\n",
      "146m 44s (- 1003m 6s) (s 255245 in e 256 12%) 4.9800\n",
      "147m 18s (- 1002m 25s) (s 256244 in e 257 12%) 5.0615\n",
      "147m 51s (- 1001m 45s) (s 257243 in e 258 12%) 5.0545\n",
      "148m 25s (- 1001m 5s) (s 258242 in e 259 12%) 5.0114\n",
      "148m 59s (- 1000m 25s) (s 259241 in e 260 12%) 5.0008\n",
      "149m 32s (- 999m 45s) (s 260240 in e 261 13%) 5.0561\n",
      "150m 6s (- 999m 4s) (s 261239 in e 262 13%) 5.0125\n",
      "150m 39s (- 998m 23s) (s 262238 in e 263 13%) 5.0421\n",
      "151m 14s (- 997m 51s) (s 263237 in e 264 13%) 5.0620\n",
      "151m 48s (- 997m 10s) (s 264236 in e 265 13%) 5.0361\n",
      "152m 21s (- 996m 29s) (s 265235 in e 266 13%) 5.0132\n",
      "152m 54s (- 995m 49s) (s 266234 in e 267 13%) 5.0040\n",
      "153m 28s (- 995m 9s) (s 267233 in e 268 13%) 4.9676\n",
      "154m 2s (- 994m 28s) (s 268232 in e 269 13%) 4.9970\n",
      "154m 35s (- 993m 48s) (s 269231 in e 270 13%) 4.9926\n",
      "155m 9s (- 993m 7s) (s 270230 in e 271 13%) 5.0238\n",
      "155m 42s (- 992m 27s) (s 271229 in e 272 13%) 4.9532\n",
      "156m 15s (- 991m 46s) (s 272228 in e 273 13%) 4.9864\n",
      "156m 49s (- 991m 6s) (s 273227 in e 274 13%) 5.0634\n",
      "157m 23s (- 990m 30s) (s 274226 in e 275 13%) 5.0320\n",
      "157m 57s (- 989m 50s) (s 275225 in e 276 13%) 5.0048\n",
      "158m 30s (- 989m 10s) (s 276224 in e 277 13%) 4.9686\n",
      "159m 3s (- 988m 29s) (s 277223 in e 278 13%) 4.9523\n",
      "159m 37s (- 987m 49s) (s 278222 in e 279 13%) 5.0212\n",
      "160m 10s (- 987m 9s) (s 279221 in e 280 13%) 5.0203\n",
      "160m 44s (- 986m 29s) (s 280220 in e 281 14%) 5.0038\n",
      "161m 17s (- 985m 49s) (s 281219 in e 282 14%) 5.0233\n",
      "161m 51s (- 985m 9s) (s 282218 in e 283 14%) 5.0664\n",
      "162m 24s (- 984m 30s) (s 283217 in e 284 14%) 5.0212\n",
      "162m 58s (- 983m 50s) (s 284216 in e 285 14%) 5.0193\n",
      "163m 31s (- 983m 10s) (s 285215 in e 286 14%) 5.0253\n",
      "164m 5s (- 982m 31s) (s 286214 in e 287 14%) 5.0035\n",
      "164m 38s (- 981m 52s) (s 287213 in e 288 14%) 4.9565\n",
      "165m 12s (- 981m 12s) (s 288212 in e 289 14%) 4.9593\n",
      "165m 45s (- 980m 33s) (s 289211 in e 290 14%) 5.0088\n",
      "166m 19s (- 979m 53s) (s 290210 in e 291 14%) 4.9667\n",
      "166m 52s (- 979m 14s) (s 291209 in e 292 14%) 4.9687\n",
      "167m 26s (- 978m 34s) (s 292208 in e 293 14%) 5.0346\n",
      "167m 59s (- 977m 54s) (s 293207 in e 294 14%) 5.0113\n",
      "168m 33s (- 977m 16s) (s 294206 in e 295 14%) 4.9680\n",
      "169m 6s (- 976m 37s) (s 295205 in e 296 14%) 4.9697\n",
      "169m 40s (- 975m 59s) (s 296204 in e 297 14%) 4.9783\n",
      "170m 13s (- 975m 19s) (s 297203 in e 298 14%) 4.9539\n",
      "170m 47s (- 974m 40s) (s 298202 in e 299 14%) 5.0172\n",
      "171m 20s (- 974m 1s) (s 299201 in e 300 14%) 4.9919\n",
      "171m 54s (- 973m 21s) (s 300200 in e 301 15%) 5.0088\n",
      "172m 27s (- 972m 42s) (s 301199 in e 302 15%) 5.0039\n",
      "173m 1s (- 972m 3s) (s 302198 in e 303 15%) 4.9949\n",
      "173m 35s (- 971m 28s) (s 303197 in e 304 15%) 5.0081\n",
      "174m 8s (- 970m 48s) (s 304196 in e 305 15%) 4.9979\n",
      "174m 42s (- 970m 10s) (s 305195 in e 306 15%) 5.0161\n",
      "175m 15s (- 969m 31s) (s 306194 in e 307 15%) 4.9771\n",
      "175m 49s (- 968m 52s) (s 307193 in e 308 15%) 4.9372\n",
      "176m 22s (- 968m 13s) (s 308192 in e 309 15%) 4.9930\n",
      "176m 56s (- 967m 35s) (s 309191 in e 310 15%) 4.9901\n",
      "177m 29s (- 966m 56s) (s 310190 in e 311 15%) 5.0016\n",
      "178m 3s (- 966m 17s) (s 311189 in e 312 15%) 4.9913\n",
      "178m 36s (- 965m 39s) (s 312188 in e 313 15%) 4.9887\n",
      "179m 10s (- 965m 0s) (s 313187 in e 314 15%) 4.9878\n",
      "179m 43s (- 964m 22s) (s 314186 in e 315 15%) 4.9581\n",
      "180m 17s (- 963m 43s) (s 315185 in e 316 15%) 5.0059\n",
      "180m 50s (- 963m 5s) (s 316184 in e 317 15%) 4.9279\n",
      "181m 24s (- 962m 26s) (s 317183 in e 318 15%) 4.9565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181m 57s (- 961m 47s) (s 318182 in e 319 15%) 4.9800\n",
      "182m 31s (- 961m 8s) (s 319181 in e 320 15%) 4.9563\n",
      "183m 4s (- 960m 30s) (s 320180 in e 321 16%) 4.9710\n",
      "183m 38s (- 959m 52s) (s 321179 in e 322 16%) 4.9524\n",
      "184m 11s (- 959m 14s) (s 322178 in e 323 16%) 4.9525\n",
      "184m 45s (- 958m 36s) (s 323177 in e 324 16%) 4.9057\n",
      "185m 18s (- 957m 59s) (s 324176 in e 325 16%) 4.9895\n",
      "185m 52s (- 957m 20s) (s 325175 in e 326 16%) 4.9402\n",
      "186m 25s (- 956m 41s) (s 326174 in e 327 16%) 5.0129\n",
      "186m 59s (- 956m 3s) (s 327173 in e 328 16%) 5.0284\n",
      "187m 32s (- 955m 25s) (s 328172 in e 329 16%) 4.9411\n",
      "188m 6s (- 954m 47s) (s 329171 in e 330 16%) 4.8907\n",
      "188m 39s (- 954m 8s) (s 330170 in e 331 16%) 4.9872\n",
      "189m 13s (- 953m 31s) (s 331169 in e 332 16%) 4.9670\n",
      "189m 46s (- 952m 53s) (s 332168 in e 333 16%) 4.9882\n",
      "190m 20s (- 952m 15s) (s 333167 in e 334 16%) 4.9770\n",
      "190m 53s (- 951m 37s) (s 334166 in e 335 16%) 4.9771\n",
      "191m 27s (- 950m 58s) (s 335165 in e 336 16%) 4.9453\n",
      "192m 0s (- 950m 20s) (s 336164 in e 337 16%) 4.9418\n",
      "192m 33s (- 949m 42s) (s 337163 in e 338 16%) 4.9740\n",
      "193m 8s (- 949m 7s) (s 338162 in e 339 16%) 5.0029\n",
      "193m 41s (- 948m 29s) (s 339161 in e 340 16%) 5.0176\n",
      "194m 15s (- 947m 51s) (s 340160 in e 341 17%) 4.9484\n",
      "194m 48s (- 947m 14s) (s 341159 in e 342 17%) 4.9558\n",
      "195m 22s (- 946m 36s) (s 342158 in e 343 17%) 4.9722\n",
      "195m 55s (- 945m 58s) (s 343157 in e 344 17%) 5.0284\n",
      "196m 29s (- 945m 21s) (s 344156 in e 345 17%) 4.9600\n",
      "197m 2s (- 944m 44s) (s 345155 in e 346 17%) 4.9758\n",
      "197m 36s (- 944m 6s) (s 346154 in e 347 17%) 4.9585\n",
      "198m 9s (- 943m 29s) (s 347153 in e 348 17%) 4.9742\n",
      "198m 43s (- 942m 51s) (s 348152 in e 349 17%) 4.9782\n",
      "199m 16s (- 942m 13s) (s 349151 in e 350 17%) 4.9844\n",
      "199m 50s (- 941m 36s) (s 350150 in e 351 17%) 4.9388\n",
      "200m 23s (- 940m 58s) (s 351149 in e 352 17%) 4.8902\n",
      "200m 57s (- 940m 21s) (s 352148 in e 353 17%) 4.9374\n",
      "201m 30s (- 939m 43s) (s 353147 in e 354 17%) 4.9812\n",
      "202m 4s (- 939m 6s) (s 354146 in e 355 17%) 4.9754\n",
      "202m 37s (- 938m 28s) (s 355145 in e 356 17%) 4.9499\n",
      "203m 11s (- 937m 51s) (s 356144 in e 357 17%) 4.9391\n",
      "203m 44s (- 937m 14s) (s 357143 in e 358 17%) 4.9842\n",
      "204m 18s (- 936m 37s) (s 358142 in e 359 17%) 4.9650\n",
      "204m 51s (- 935m 59s) (s 359141 in e 360 17%) 4.9464\n",
      "205m 25s (- 935m 22s) (s 360140 in e 361 18%) 4.9530\n",
      "205m 58s (- 934m 45s) (s 361139 in e 362 18%) 4.9210\n",
      "206m 32s (- 934m 7s) (s 362138 in e 363 18%) 4.9805\n",
      "207m 5s (- 933m 30s) (s 363137 in e 364 18%) 4.9597\n",
      "207m 39s (- 932m 52s) (s 364136 in e 365 18%) 4.9711\n",
      "208m 12s (- 932m 16s) (s 365135 in e 366 18%) 4.9350\n",
      "208m 46s (- 931m 39s) (s 366134 in e 367 18%) 4.9552\n",
      "209m 19s (- 931m 2s) (s 367133 in e 368 18%) 4.9077\n",
      "209m 53s (- 930m 24s) (s 368132 in e 369 18%) 4.9345\n",
      "210m 27s (- 929m 47s) (s 369131 in e 370 18%) 4.9694\n",
      "211m 0s (- 929m 11s) (s 370130 in e 371 18%) 4.9145\n",
      "211m 34s (- 928m 34s) (s 371129 in e 372 18%) 4.9978\n",
      "212m 7s (- 927m 56s) (s 372128 in e 373 18%) 4.9873\n",
      "212m 41s (- 927m 21s) (s 373127 in e 374 18%) 4.9631\n",
      "213m 16s (- 926m 52s) (s 374126 in e 375 18%) 4.9523\n",
      "213m 50s (- 926m 15s) (s 375125 in e 376 18%) 4.9317\n",
      "214m 23s (- 925m 38s) (s 376124 in e 377 18%) 4.9964\n",
      "214m 57s (- 925m 1s) (s 377123 in e 378 18%) 4.9545\n",
      "215m 30s (- 924m 24s) (s 378122 in e 379 18%) 4.9477\n",
      "216m 4s (- 923m 47s) (s 379121 in e 380 18%) 4.9866\n",
      "216m 38s (- 923m 11s) (s 380120 in e 381 19%) 5.0173\n",
      "217m 11s (- 922m 33s) (s 381119 in e 382 19%) 4.9574\n",
      "217m 44s (- 921m 56s) (s 382118 in e 383 19%) 4.9792\n",
      "218m 18s (- 921m 19s) (s 383117 in e 384 19%) 4.9692\n",
      "218m 51s (- 920m 42s) (s 384116 in e 385 19%) 4.9487\n",
      "219m 25s (- 920m 5s) (s 385115 in e 386 19%) 4.9461\n",
      "219m 58s (- 919m 29s) (s 386114 in e 387 19%) 4.9714\n",
      "220m 32s (- 918m 52s) (s 387113 in e 388 19%) 4.9482\n",
      "221m 5s (- 918m 15s) (s 388112 in e 389 19%) 4.9636\n",
      "221m 39s (- 917m 38s) (s 389111 in e 390 19%) 4.9535\n",
      "222m 12s (- 917m 1s) (s 390110 in e 391 19%) 4.9670\n",
      "222m 46s (- 916m 24s) (s 391109 in e 392 19%) 4.9744\n",
      "223m 19s (- 915m 47s) (s 392108 in e 393 19%) 4.9079\n",
      "223m 53s (- 915m 11s) (s 393107 in e 394 19%) 5.0129\n",
      "224m 26s (- 914m 34s) (s 394106 in e 395 19%) 4.9618\n",
      "225m 0s (- 913m 58s) (s 395105 in e 396 19%) 4.9210\n",
      "225m 34s (- 913m 22s) (s 396104 in e 397 19%) 4.9390\n",
      "226m 7s (- 912m 45s) (s 397103 in e 398 19%) 4.9747\n",
      "226m 41s (- 912m 8s) (s 398102 in e 399 19%) 4.9746\n",
      "227m 14s (- 911m 31s) (s 399101 in e 400 19%) 4.9630\n",
      "227m 47s (- 910m 54s) (s 400100 in e 401 20%) 4.9516\n",
      "228m 21s (- 910m 17s) (s 401099 in e 402 20%) 4.9478\n",
      "228m 55s (- 909m 42s) (s 402098 in e 403 20%) 4.9015\n",
      "229m 29s (- 909m 7s) (s 403097 in e 404 20%) 4.9230\n",
      "230m 2s (- 908m 31s) (s 404096 in e 405 20%) 4.9644\n",
      "230m 36s (- 907m 54s) (s 405095 in e 406 20%) 4.9931\n",
      "231m 9s (- 907m 17s) (s 406094 in e 407 20%) 4.9371\n",
      "231m 43s (- 906m 41s) (s 407093 in e 408 20%) 4.8981\n",
      "232m 16s (- 906m 4s) (s 408092 in e 409 20%) 4.9664\n",
      "232m 50s (- 905m 27s) (s 409091 in e 410 20%) 4.9878\n",
      "233m 23s (- 904m 51s) (s 410090 in e 411 20%) 4.9532\n",
      "233m 57s (- 904m 15s) (s 411089 in e 412 20%) 4.8970\n",
      "234m 30s (- 903m 38s) (s 412088 in e 413 20%) 4.9402\n",
      "235m 4s (- 903m 2s) (s 413087 in e 414 20%) 4.9686\n",
      "235m 37s (- 902m 26s) (s 414086 in e 415 20%) 4.9621\n",
      "236m 11s (- 901m 49s) (s 415085 in e 416 20%) 4.9232\n",
      "236m 44s (- 901m 13s) (s 416084 in e 417 20%) 4.9242\n",
      "237m 18s (- 900m 36s) (s 417083 in e 418 20%) 4.9329\n",
      "237m 51s (- 900m 0s) (s 418082 in e 419 20%) 4.9566\n",
      "238m 25s (- 899m 24s) (s 419081 in e 420 20%) 4.9254\n",
      "238m 58s (- 898m 47s) (s 420080 in e 421 21%) 4.9620\n",
      "239m 32s (- 898m 11s) (s 421079 in e 422 21%) 4.9290\n",
      "240m 5s (- 897m 35s) (s 422078 in e 423 21%) 4.9319\n",
      "240m 39s (- 896m 58s) (s 423077 in e 424 21%) 4.9522\n",
      "241m 12s (- 896m 22s) (s 424076 in e 425 21%) 4.9340\n",
      "241m 46s (- 895m 46s) (s 425075 in e 426 21%) 4.9235\n",
      "242m 19s (- 895m 9s) (s 426074 in e 427 21%) 4.9259\n",
      "242m 53s (- 894m 33s) (s 427073 in e 428 21%) 4.9221\n",
      "243m 26s (- 893m 57s) (s 428072 in e 429 21%) 4.9096\n",
      "244m 0s (- 893m 22s) (s 429071 in e 430 21%) 4.9357\n",
      "244m 34s (- 892m 46s) (s 430070 in e 431 21%) 4.9450\n",
      "245m 7s (- 892m 10s) (s 431069 in e 432 21%) 4.9509\n",
      "245m 41s (- 891m 33s) (s 432068 in e 433 21%) 4.9429\n",
      "246m 16s (- 891m 3s) (s 433067 in e 434 21%) 4.9343\n",
      "246m 49s (- 890m 27s) (s 434066 in e 435 21%) 4.9579\n",
      "247m 23s (- 889m 51s) (s 435065 in e 436 21%) 4.9454\n",
      "247m 56s (- 889m 15s) (s 436064 in e 437 21%) 4.9787\n",
      "248m 30s (- 888m 38s) (s 437063 in e 438 21%) 4.9129\n",
      "249m 3s (- 888m 3s) (s 438062 in e 439 21%) 4.9406\n",
      "249m 37s (- 887m 27s) (s 439061 in e 440 21%) 4.8922\n",
      "250m 10s (- 886m 51s) (s 440060 in e 441 22%) 4.9430\n",
      "250m 44s (- 886m 15s) (s 441059 in e 442 22%) 4.8983\n",
      "251m 17s (- 885m 39s) (s 442058 in e 443 22%) 4.8990\n",
      "251m 51s (- 885m 2s) (s 443057 in e 444 22%) 4.8576\n",
      "252m 24s (- 884m 26s) (s 444056 in e 445 22%) 4.8762\n",
      "252m 58s (- 883m 50s) (s 445055 in e 446 22%) 4.9179\n",
      "253m 31s (- 883m 14s) (s 446054 in e 447 22%) 4.8718\n",
      "254m 5s (- 882m 38s) (s 447053 in e 448 22%) 4.9259\n",
      "254m 38s (- 882m 2s) (s 448052 in e 449 22%) 4.9447\n",
      "255m 12s (- 881m 26s) (s 449051 in e 450 22%) 4.9116\n",
      "255m 45s (- 880m 50s) (s 450050 in e 451 22%) 4.9281\n",
      "256m 19s (- 880m 14s) (s 451049 in e 452 22%) 4.8912\n",
      "256m 52s (- 879m 38s) (s 452048 in e 453 22%) 4.9657\n",
      "257m 26s (- 879m 2s) (s 453047 in e 454 22%) 4.8902\n",
      "257m 59s (- 878m 26s) (s 454046 in e 455 22%) 4.9686\n",
      "258m 33s (- 877m 50s) (s 455045 in e 456 22%) 4.9172\n",
      "259m 6s (- 877m 14s) (s 456044 in e 457 22%) 4.9474\n",
      "259m 40s (- 876m 38s) (s 457043 in e 458 22%) 4.9480\n",
      "260m 13s (- 876m 1s) (s 458042 in e 459 22%) 4.8948\n",
      "260m 47s (- 875m 25s) (s 459041 in e 460 22%) 4.8819\n",
      "261m 20s (- 874m 49s) (s 460040 in e 461 23%) 4.9050\n",
      "261m 54s (- 874m 13s) (s 461039 in e 462 23%) 4.9283\n",
      "262m 27s (- 873m 37s) (s 462038 in e 463 23%) 4.9810\n",
      "263m 1s (- 873m 2s) (s 463037 in e 464 23%) 4.9549\n",
      "263m 35s (- 872m 28s) (s 464036 in e 465 23%) 4.9313\n",
      "264m 8s (- 871m 52s) (s 465035 in e 466 23%) 4.9196\n",
      "264m 42s (- 871m 16s) (s 466034 in e 467 23%) 4.9517\n",
      "265m 15s (- 870m 41s) (s 467033 in e 468 23%) 4.9547\n",
      "265m 49s (- 870m 5s) (s 468032 in e 469 23%) 4.9218\n",
      "266m 22s (- 869m 29s) (s 469031 in e 470 23%) 4.9016\n",
      "266m 56s (- 868m 54s) (s 470030 in e 471 23%) 4.8897\n",
      "267m 29s (- 868m 18s) (s 471029 in e 472 23%) 4.9256\n",
      "268m 3s (- 867m 42s) (s 472028 in e 473 23%) 4.8941\n",
      "268m 36s (- 867m 6s) (s 473027 in e 474 23%) 4.9412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269m 10s (- 866m 30s) (s 474026 in e 475 23%) 4.9862\n",
      "269m 44s (- 865m 55s) (s 475025 in e 476 23%) 4.9405\n",
      "270m 17s (- 865m 19s) (s 476024 in e 477 23%) 4.8781\n",
      "270m 51s (- 864m 44s) (s 477023 in e 478 23%) 4.9175\n",
      "271m 24s (- 864m 8s) (s 478022 in e 479 23%) 4.9458\n",
      "271m 58s (- 863m 33s) (s 479021 in e 480 23%) 4.9665\n",
      "272m 31s (- 862m 57s) (s 480020 in e 481 24%) 4.9322\n",
      "273m 5s (- 862m 22s) (s 481019 in e 482 24%) 4.9563\n",
      "273m 38s (- 861m 46s) (s 482018 in e 483 24%) 4.9008\n",
      "274m 12s (- 861m 11s) (s 483017 in e 484 24%) 4.9177\n",
      "274m 46s (- 860m 36s) (s 484016 in e 485 24%) 4.9258\n",
      "275m 19s (- 860m 0s) (s 485015 in e 486 24%) 4.9951\n",
      "275m 53s (- 859m 25s) (s 486014 in e 487 24%) 4.9452\n",
      "276m 26s (- 858m 49s) (s 487013 in e 488 24%) 4.9059\n",
      "277m 0s (- 858m 13s) (s 488012 in e 489 24%) 4.9551\n",
      "277m 33s (- 857m 37s) (s 489011 in e 490 24%) 4.9020\n",
      "278m 7s (- 857m 2s) (s 490010 in e 491 24%) 4.9075\n",
      "278m 40s (- 856m 26s) (s 491009 in e 492 24%) 4.9377\n",
      "279m 14s (- 855m 50s) (s 492008 in e 493 24%) 4.9206\n",
      "279m 47s (- 855m 15s) (s 493007 in e 494 24%) 4.9602\n",
      "280m 21s (- 854m 40s) (s 494006 in e 495 24%) 4.9375\n",
      "280m 54s (- 854m 4s) (s 495005 in e 496 24%) 4.9076\n",
      "281m 28s (- 853m 28s) (s 496004 in e 497 24%) 4.9131\n",
      "282m 1s (- 852m 53s) (s 497003 in e 498 24%) 4.9417\n",
      "282m 35s (- 852m 17s) (s 498002 in e 499 24%) 4.9197\n",
      "283m 8s (- 851m 42s) (s 499001 in e 500 24%) 4.9172\n",
      "283m 43s (- 851m 9s) (s 500000 in e 501 25%) 4.8740\n",
      "284m 16s (- 850m 33s) (s 500999 in e 502 25%) 4.8922\n",
      "284m 50s (- 849m 58s) (s 501998 in e 503 25%) 4.9378\n",
      "285m 23s (- 849m 23s) (s 502997 in e 504 25%) 4.9077\n",
      "285m 57s (- 848m 47s) (s 503996 in e 505 25%) 4.9482\n",
      "286m 30s (- 848m 12s) (s 504995 in e 506 25%) 4.8937\n",
      "287m 4s (- 847m 36s) (s 505994 in e 507 25%) 4.9418\n",
      "287m 37s (- 847m 1s) (s 506993 in e 508 25%) 4.9431\n",
      "288m 11s (- 846m 25s) (s 507992 in e 509 25%) 4.9512\n",
      "288m 44s (- 845m 50s) (s 508991 in e 510 25%) 4.9361\n",
      "289m 18s (- 845m 15s) (s 509990 in e 511 25%) 4.9334\n",
      "289m 52s (- 844m 39s) (s 510989 in e 512 25%) 4.9096\n",
      "290m 25s (- 844m 4s) (s 511988 in e 513 25%) 4.9153\n",
      "290m 59s (- 843m 29s) (s 512987 in e 514 25%) 4.9696\n",
      "291m 32s (- 842m 53s) (s 513986 in e 515 25%) 4.9374\n",
      "292m 6s (- 842m 18s) (s 514985 in e 516 25%) 4.9357\n",
      "292m 39s (- 841m 42s) (s 515984 in e 517 25%) 4.9204\n",
      "293m 13s (- 841m 7s) (s 516983 in e 518 25%) 4.9342\n",
      "293m 46s (- 840m 32s) (s 517982 in e 519 25%) 4.9438\n",
      "294m 20s (- 839m 57s) (s 518981 in e 520 25%) 4.9196\n",
      "294m 54s (- 839m 23s) (s 519980 in e 521 25%) 4.9831\n",
      "295m 27s (- 838m 47s) (s 520979 in e 522 26%) 4.9334\n",
      "296m 1s (- 838m 12s) (s 521978 in e 523 26%) 4.9614\n",
      "296m 34s (- 837m 36s) (s 522977 in e 524 26%) 4.9361\n",
      "297m 8s (- 837m 1s) (s 523976 in e 525 26%) 4.8846\n",
      "297m 41s (- 836m 26s) (s 524975 in e 526 26%) 4.8852\n",
      "298m 15s (- 835m 51s) (s 525974 in e 527 26%) 4.9734\n",
      "298m 48s (- 835m 16s) (s 526973 in e 528 26%) 4.9385\n",
      "299m 22s (- 834m 40s) (s 527972 in e 529 26%) 4.9034\n",
      "299m 56s (- 834m 5s) (s 528971 in e 530 26%) 4.9375\n",
      "300m 29s (- 833m 30s) (s 529970 in e 531 26%) 5.0035\n",
      "301m 3s (- 832m 56s) (s 530969 in e 532 26%) 4.8753\n",
      "301m 37s (- 832m 22s) (s 531968 in e 533 26%) 4.9671\n",
      "302m 11s (- 831m 47s) (s 532967 in e 534 26%) 4.9934\n",
      "302m 45s (- 831m 13s) (s 533966 in e 535 26%) 4.9414\n",
      "303m 19s (- 830m 39s) (s 534965 in e 536 26%) 4.8798\n",
      "303m 52s (- 830m 4s) (s 535964 in e 537 26%) 4.9053\n",
      "304m 26s (- 829m 30s) (s 536963 in e 538 26%) 4.9224\n",
      "305m 0s (- 828m 56s) (s 537962 in e 539 26%) 4.9090\n",
      "305m 34s (- 828m 22s) (s 538961 in e 540 26%) 4.9333\n",
      "306m 8s (- 827m 48s) (s 539960 in e 541 26%) 4.9431\n",
      "306m 42s (- 827m 14s) (s 540959 in e 542 27%) 4.9381\n",
      "307m 16s (- 826m 39s) (s 541958 in e 543 27%) 5.0046\n",
      "307m 50s (- 826m 5s) (s 542957 in e 544 27%) 4.9132\n",
      "308m 25s (- 825m 36s) (s 543956 in e 545 27%) 4.9255\n",
      "308m 59s (- 825m 2s) (s 544955 in e 546 27%) 4.9260\n",
      "309m 33s (- 824m 27s) (s 545954 in e 547 27%) 4.9091\n",
      "310m 7s (- 823m 54s) (s 546953 in e 548 27%) 4.8976\n",
      "310m 41s (- 823m 20s) (s 547952 in e 549 27%) 4.9355\n",
      "311m 16s (- 822m 46s) (s 548951 in e 550 27%) 4.9435\n",
      "311m 49s (- 822m 12s) (s 549950 in e 551 27%) 4.8946\n",
      "312m 23s (- 821m 38s) (s 550949 in e 552 27%) 4.9247\n",
      "312m 57s (- 821m 4s) (s 551948 in e 553 27%) 4.9459\n",
      "313m 31s (- 820m 30s) (s 552947 in e 554 27%) 4.9453\n",
      "314m 5s (- 819m 56s) (s 553946 in e 555 27%) 4.9454\n",
      "314m 39s (- 819m 22s) (s 554945 in e 556 27%) 4.8998\n",
      "315m 13s (- 818m 48s) (s 555944 in e 557 27%) 4.9329\n",
      "315m 47s (- 818m 14s) (s 556943 in e 558 27%) 4.9248\n",
      "316m 22s (- 817m 41s) (s 557942 in e 559 27%) 4.9201\n",
      "316m 56s (- 817m 7s) (s 558941 in e 560 27%) 4.8907\n",
      "317m 30s (- 816m 33s) (s 559940 in e 561 27%) 4.9573\n",
      "318m 4s (- 815m 59s) (s 560939 in e 562 28%) 4.8759\n",
      "318m 37s (- 815m 24s) (s 561938 in e 563 28%) 4.9454\n",
      "319m 12s (- 814m 51s) (s 562937 in e 564 28%) 4.9479\n",
      "319m 46s (- 814m 17s) (s 563936 in e 565 28%) 4.9120\n",
      "320m 20s (- 813m 43s) (s 564935 in e 566 28%) 4.9011\n",
      "320m 54s (- 813m 9s) (s 565934 in e 567 28%) 4.9098\n",
      "321m 27s (- 812m 35s) (s 566933 in e 568 28%) 4.9226\n",
      "322m 1s (- 812m 1s) (s 567932 in e 569 28%) 4.9091\n",
      "322m 35s (- 811m 26s) (s 568931 in e 570 28%) 4.9252\n",
      "323m 9s (- 810m 52s) (s 569930 in e 571 28%) 4.9521\n",
      "323m 43s (- 810m 18s) (s 570929 in e 572 28%) 4.9586\n",
      "324m 17s (- 809m 44s) (s 571928 in e 573 28%) 4.9705\n",
      "324m 51s (- 809m 9s) (s 572927 in e 574 28%) 4.9890\n",
      "325m 25s (- 808m 35s) (s 573926 in e 575 28%) 4.9215\n",
      "325m 59s (- 808m 1s) (s 574925 in e 576 28%) 4.9739\n",
      "326m 33s (- 807m 27s) (s 575924 in e 577 28%) 4.9695\n",
      "327m 7s (- 806m 53s) (s 576923 in e 578 28%) 4.9515\n",
      "327m 41s (- 806m 19s) (s 577922 in e 579 28%) 4.9840\n",
      "328m 14s (- 805m 45s) (s 578921 in e 580 28%) 4.9312\n",
      "328m 48s (- 805m 11s) (s 579920 in e 581 28%) 4.8714\n",
      "329m 22s (- 804m 37s) (s 580919 in e 582 29%) 4.9177\n",
      "329m 56s (- 804m 3s) (s 581918 in e 583 29%) 4.9372\n",
      "330m 30s (- 803m 29s) (s 582917 in e 584 29%) 4.9430\n",
      "331m 4s (- 802m 55s) (s 583916 in e 585 29%) 4.9637\n",
      "331m 38s (- 802m 20s) (s 584915 in e 586 29%) 4.9438\n",
      "332m 12s (- 801m 45s) (s 585914 in e 587 29%) 4.8834\n",
      "332m 45s (- 801m 10s) (s 586913 in e 588 29%) 4.9411\n",
      "333m 19s (- 800m 35s) (s 587912 in e 589 29%) 4.8737\n",
      "333m 52s (- 799m 59s) (s 588911 in e 590 29%) 4.9213\n",
      "334m 26s (- 799m 25s) (s 589910 in e 591 29%) 4.9376\n",
      "334m 59s (- 798m 50s) (s 590909 in e 592 29%) 4.9344\n",
      "335m 33s (- 798m 15s) (s 591908 in e 593 29%) 4.9564\n",
      "336m 6s (- 797m 39s) (s 592907 in e 594 29%) 4.9177\n",
      "336m 40s (- 797m 4s) (s 593906 in e 595 29%) 4.9778\n",
      "337m 13s (- 796m 29s) (s 594905 in e 596 29%) 4.9469\n",
      "337m 47s (- 795m 54s) (s 595904 in e 597 29%) 4.9612\n",
      "338m 20s (- 795m 19s) (s 596903 in e 598 29%) 4.9494\n",
      "338m 54s (- 794m 44s) (s 597902 in e 599 29%) 4.8904\n",
      "339m 27s (- 794m 9s) (s 598901 in e 600 29%) 4.9748\n",
      "340m 1s (- 793m 34s) (s 599900 in e 601 29%) 4.9346\n",
      "340m 34s (- 792m 59s) (s 600899 in e 602 30%) 4.9136\n",
      "341m 9s (- 792m 26s) (s 601898 in e 603 30%) 4.9342\n",
      "341m 42s (- 791m 51s) (s 602897 in e 604 30%) 4.9781\n",
      "342m 16s (- 791m 16s) (s 603896 in e 605 30%) 4.9126\n",
      "342m 49s (- 790m 41s) (s 604895 in e 606 30%) 4.9193\n",
      "343m 23s (- 790m 6s) (s 605894 in e 607 30%) 4.9136\n",
      "343m 56s (- 789m 31s) (s 606893 in e 608 30%) 4.9215\n",
      "344m 30s (- 788m 57s) (s 607892 in e 609 30%) 4.9018\n",
      "345m 4s (- 788m 22s) (s 608891 in e 610 30%) 4.9012\n",
      "345m 37s (- 787m 47s) (s 609890 in e 611 30%) 4.9472\n",
      "346m 11s (- 787m 12s) (s 610889 in e 612 30%) 4.9497\n",
      "346m 44s (- 786m 37s) (s 611888 in e 613 30%) 4.9152\n",
      "347m 18s (- 786m 2s) (s 612887 in e 614 30%) 4.9298\n",
      "347m 51s (- 785m 27s) (s 613886 in e 615 30%) 4.9397\n",
      "348m 25s (- 784m 52s) (s 614885 in e 616 30%) 4.9602\n",
      "348m 58s (- 784m 17s) (s 615884 in e 617 30%) 4.9518\n",
      "349m 32s (- 783m 42s) (s 616883 in e 618 30%) 4.9759\n",
      "350m 5s (- 783m 7s) (s 617882 in e 619 30%) 4.9356\n",
      "350m 39s (- 782m 32s) (s 618881 in e 620 30%) 4.9354\n",
      "351m 12s (- 781m 57s) (s 619880 in e 621 30%) 4.9357\n",
      "351m 46s (- 781m 22s) (s 620879 in e 622 31%) 4.9160\n",
      "352m 19s (- 780m 47s) (s 621878 in e 623 31%) 4.9277\n",
      "352m 53s (- 780m 12s) (s 622877 in e 624 31%) 4.8763\n",
      "353m 27s (- 779m 37s) (s 623876 in e 625 31%) 4.8757\n",
      "354m 0s (- 779m 2s) (s 624875 in e 626 31%) 4.9435\n",
      "354m 33s (- 778m 27s) (s 625874 in e 627 31%) 4.9337\n",
      "355m 7s (- 777m 53s) (s 626873 in e 628 31%) 4.9039\n",
      "355m 41s (- 777m 17s) (s 627872 in e 629 31%) 4.9930\n",
      "356m 14s (- 776m 43s) (s 628871 in e 630 31%) 4.9535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356m 47s (- 776m 7s) (s 629870 in e 631 31%) 4.9590\n",
      "357m 21s (- 775m 33s) (s 630869 in e 632 31%) 4.9174\n",
      "357m 55s (- 774m 59s) (s 631868 in e 633 31%) 4.9412\n",
      "358m 29s (- 774m 24s) (s 632867 in e 634 31%) 4.9629\n",
      "359m 2s (- 773m 50s) (s 633866 in e 635 31%) 4.9321\n",
      "359m 36s (- 773m 15s) (s 634865 in e 636 31%) 4.9000\n",
      "360m 9s (- 772m 40s) (s 635864 in e 637 31%) 4.9152\n",
      "360m 43s (- 772m 5s) (s 636863 in e 638 31%) 4.9536\n",
      "361m 16s (- 771m 30s) (s 637862 in e 639 31%) 4.9199\n",
      "361m 50s (- 770m 55s) (s 638861 in e 640 31%) 4.9500\n",
      "362m 23s (- 770m 20s) (s 639860 in e 641 31%) 4.9614\n",
      "362m 57s (- 769m 45s) (s 640859 in e 642 32%) 4.9054\n",
      "363m 30s (- 769m 10s) (s 641858 in e 643 32%) 4.9628\n",
      "364m 4s (- 768m 35s) (s 642857 in e 644 32%) 4.9790\n",
      "364m 37s (- 768m 0s) (s 643856 in e 645 32%) 4.9832\n",
      "365m 11s (- 767m 26s) (s 644855 in e 646 32%) 4.9550\n",
      "365m 44s (- 766m 51s) (s 645854 in e 647 32%) 4.8953\n",
      "366m 19s (- 766m 19s) (s 646853 in e 648 32%) 4.9277\n",
      "366m 54s (- 765m 46s) (s 647852 in e 649 32%) 4.9612\n",
      "367m 27s (- 765m 11s) (s 648851 in e 650 32%) 4.9454\n",
      "368m 1s (- 764m 36s) (s 649850 in e 651 32%) 4.9854\n",
      "368m 34s (- 764m 1s) (s 650849 in e 652 32%) 4.9499\n",
      "369m 8s (- 763m 26s) (s 651848 in e 653 32%) 4.9495\n",
      "369m 41s (- 762m 51s) (s 652847 in e 654 32%) 4.9770\n",
      "370m 19s (- 762m 26s) (s 653846 in e 655 32%) 4.9018\n"
     ]
    }
   ],
   "source": [
    "print('Is about to start training...')\n",
    "start = time.time()\n",
    "step = 1\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for it in range(n_iterations):\n",
    "        # randomly select input sequence\n",
    "        idx = random.randrange(0, n_iterations)\n",
    "\n",
    "        # convert input sequence to padded array\n",
    "        fact = pad(inputs_train[idx], tr_input_size)\n",
    "        question = pad(queries_train[idx], tr_input_size)\n",
    "        answer = answers_train[idx]\n",
    "\n",
    "        # wrap into Tensor and  Variable\n",
    "        fact = Variable(torch.LongTensor(fact))\n",
    "        question = Variable(torch.LongTensor(question))\n",
    "        answer = Variable(torch.LongTensor(answer))\n",
    "\n",
    "        # feed\n",
    "        tr_hidden_state = Variable(torch.zeros(tr_n_layers, 1, tr_hidden_size))\n",
    "        loss = train(fact, question, answer, tr_hidden_state, memory_model,\n",
    "                     answer_model, memory_optimizer, answer_optimizer, criterion)\n",
    "\n",
    "        # keep track loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if it == 0:\n",
    "            continue\n",
    "\n",
    "        if it % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (s %d in e %d %d%%) %.4f' % (time_since(\n",
    "                start, step / (n_epochs * n_iterations)), step, epoch, (step / (n_epochs * n_iterations)) * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "\n",
    "        if it % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_loss_total = 0\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "        step += 1\n",
    "\n",
    "    if epoch % save_model_every == 0:\n",
    "        torch.save({\n",
    "            'encoder': memory_model.state_dict(),\n",
    "            'decoder': answer_model.state_dict(),\n",
    "            'encoder_optimizer': memory_optimizer.state_dict(),\n",
    "            'decoder_optimizer': answer_optimizer.state_dict()\n",
    "        },\n",
    "            trained_model_path.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(fact, question, memory_model, answer_model, hidden_state, vocab):\n",
    "\n",
    "    # wrap into Tensor and  Variable\n",
    "    fact = Variable(torch.LongTensor(fact))\n",
    "    question = Variable(torch.LongTensor(question))\n",
    "\n",
    "    # orchestrate model learning\n",
    "    answer_support, hidden_state = memory_model(fact, question, hidden_state)\n",
    "    predicted_answer = answer_model(answer_support)\n",
    "\n",
    "    _, word_idx = torch.max(predicted_answer.data, 0)\n",
    "\n",
    "    return vocab[word_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert input sequence to padded array\n",
    "idx = random.randrange(0, len(inputs_test))\n",
    "fact = pad(inputs_test[idx], tr_input_size)\n",
    "question = pad(queries_test[idx], tr_input_size)\n",
    "\n",
    "# feed\n",
    "t_hidden_state = Variable(torch.zeros(tr_n_layers, 1, tr_hidden_size))\n",
    "\n",
    "predicted_answer = evaluate(\n",
    "    fact, question, memory_model, answer_model, t_hidden_state, vocab)\n",
    "\n",
    "print([''.join(vocab[x]) for x in inputs_test[idx]])\n",
    "print([''.join(vocab[x]) for x in queries_test[idx]])\n",
    "print('expected answer: [', vocab[answers_test[idx][0]],\n",
    "      '] and predicted one: [', predicted_answer, ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
