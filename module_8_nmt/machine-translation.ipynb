{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Loading data files\n",
    "\n",
    "The data for this project is a set of many thousands of English to French translation pairs from http://www.manythings.org/anki/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-01-07 02:16:00--  http://www.manythings.org/anki/ukr-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196\n",
      "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1205944 (1.1M) [application/zip]\n",
      "Saving to: 'ukr-eng.zip'\n",
      "\n",
      "ukr-eng.zip         100%[===================>]   1.15M   972KB/s    in 1.2s    \n",
      "\n",
      "2018-01-07 02:16:02 (972 KB/s) - 'ukr-eng.zip' saved [1205944/1205944]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets download data set\n",
    "!wget - c \"http://www.manythings.org/anki/ukr-eng.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4232\r\n",
      "drwxr-xr-x   5 webdizz  staff      160 Jan  7 02:16 \u001b[34m.\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  12 webdizz  staff      384 Jan  6 13:26 \u001b[34m..\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   3 webdizz  staff       96 Jan  6 13:34 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 webdizz  staff    30647 Jan  7 02:14 machine-translation.ipynb\r\n",
      "-rw-r--r--@  1 webdizz  staff  1205944 Oct 30 10:14 ukr-eng.zip\r\n"
     ]
    }
   ],
   "source": [
    "# check it was downloaded\n",
    "!ls - la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ukr-eng.zip\n",
      "  inflating: _about.txt              \n",
      "  inflating: ukr.txt                 \n",
      "total 12656\n",
      "drwxr-xr-x   7 webdizz  staff      224 Jan  7 02:16 \u001b[34m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  12 webdizz  staff      384 Jan  6 13:26 \u001b[34m..\u001b[m\u001b[m\n",
      "drwxr-xr-x   3 webdizz  staff       96 Jan  6 13:34 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\n",
      "-rw-r--r--   1 webdizz  staff     1441 Oct 30 17:14 _about.txt\n",
      "-rw-r--r--   1 webdizz  staff    30647 Jan  7 02:14 machine-translation.ipynb\n",
      "-rw-r--r--@  1 webdizz  staff  1205944 Oct 30 10:14 ukr-eng.zip\n",
      "-rw-r--r--   1 webdizz  staff  4308132 Oct 30 17:14 ukr.txt\n"
     ]
    }
   ],
   "source": [
    "# extract dataset\n",
    "!unzip - o ukr - eng.zip\n",
    "!ls - la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing words\n",
    "\n",
    "We'll need a unique index per word to use as the inputs and targets of the network later. To keep of all this we'll create an wrapper class called `Language` along with utility methods to represents **word->index** and **index->word** associations as well as count of each word, which will be useful later to replace rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'}\n",
    "        self.n_words = 2\n",
    "        self.max_sentence_length = 0\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        words_in_sentence = sentence.split(' ')\n",
    "        for word in words_in_sentence:\n",
    "            self.index_word(word)\n",
    "\n",
    "        # update max_sentence_length to later usage within Tensor\n",
    "        sentence_len = len(words_in_sentence)\n",
    "        if self.max_sentence_length < sentence_len:\n",
    "            self.max_sentence_length = sentence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and decoding files\n",
    "\n",
    "The files are all in Unicode, to simplify we will turn Unicode to ASCII, make everything lowercase and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([-;,.!?])\", r\"\", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to read file line by line and the split lines into pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang_from, lang_to):\n",
    "    print(\"Reading language lines...\")\n",
    "\n",
    "    # read file while splitting line by line\n",
    "    lines = open('ukr.txt').read().strip().split('\\n')\n",
    "\n",
    "    # split every line into pairs and normalize\n",
    "    pairs = [[normalize(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # prepare output\n",
    "    input_lang = Language(lang_from)\n",
    "    output_lang = Language(lang_to)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def prepare_data(lang_from, lang_to):\n",
    "    input_lang, output_lang, pairs = read_langs(lang_from, lang_to)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading language lines...\n",
      "Read 54635 sentence pairs\n",
      "Indexing words...\n",
      "[\"i'm wet\", 'я мокрии']\n",
      "Max sentence length 32 for en\n",
      "Max sentence length 25 for ua\n",
      "Input language number of words 8060\n"
     ]
    }
   ],
   "source": [
    "# load and prepare data\n",
    "input_lang, output_lang, pairs = prepare_data('en', 'ua')\n",
    "\n",
    "print(random.choice(pairs))\n",
    "print(\"Max sentence length %d for en\" % input_lang.max_sentence_length)\n",
    "print(\"Max sentence length %d for ua\" % output_lang.max_sentence_length)\n",
    "print('Input language number of words', input_lang.n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Turning training data to Tensors/Variables\n",
    "\n",
    "To train we need to turn sentences into something the neural network can understand, which of course means numbers.\n",
    "Each sentence will be split into words and turned into a `Tensor`, where each words is replaced with the index (from the `Language` indexes made earlier). While creating these tensors we will also append the `EOS` token to signal that the sentence is over.\n",
    "\n",
    "![sentence as word index representation](https://camo.githubusercontent.com/f6702e41fb7582581c82be688c791416de11f761/68747470733a2f2f692e696d6775722e636f6d2f4c7a6f637047482e706e67)\n",
    "\n",
    "Trainable [PyTorch](http://pytorch.org/) modules take `Variable` as input, rather than plain `Tensor`s. A `Variable` is basically a `Tensor` that is able to keep track of the graph state, which is what makes `autograd` (automatic calculation of backwards gradients for backpropagation to work) possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    variable = Variable(torch.LongTensor([indexes])).view(-1)\n",
    "    return variable\n",
    "\n",
    "\n",
    "def variables_from_pair(input_lang, output_lang, pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    output_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, output_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "    44\n",
       "  1490\n",
       "    96\n",
       "   731\n",
       "  1792\n",
       "     1\n",
       " [torch.LongTensor of size 6], Variable containing:\n",
       "     81\n",
       "     12\n",
       "    808\n",
       "     70\n",
       "  12130\n",
       "      1\n",
       " [torch.LongTensor of size 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables_from_pair(input_lang, output_lang, pairs[30000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "The `Encoder` of a **seq2seq** network is a **RNN** that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Encoder diagram](https://github.com/spro/practical-pytorch/raw/89d8ad57f9570927ae869ec2cc6d90e9a7b38bb5/seq2seq-translation/images/encoder-network.png \"tooltip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        # define network hyperparameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # define network architecture\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, input_words, hidden_state):\n",
    "        seq_len = len(input_words)\n",
    "        embedded = self.embedding(input_words).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden_state)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder\n",
    "\n",
    "In summary out `Decoder` should consist of four main parts - an embedding layer turning an iput words into a vector; a layer to calculate the attention energy per encoder output; a RNN layer; and an output layer.\n",
    "\n",
    "The `Decoder`'s inputs are the last RNN hidden state, last output and all encoder output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `Attention` model following [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) by Luong et al. where there is a description of few more attention models that offer improvements and simplifications. They describe a few \"global attention\" models, the description between them being the way attention scores are calculated.\n",
    "\n",
    "The general form of the attention calculation relies on the target (decoder) side hidden state and corresponding source (encoder) side state, normalized over all states to get values summing to 1 e.g. by application of `softmax` function.\n",
    "\n",
    "The specific `score` function that compares two states is either `dot`, a sample dot product between the states; `general`, a dot product between the decoder hidden state and a linear transform of the encoder state; or `concat`, a dot product between a new parameter and a linear transform of the states concatenated together.\n",
    "\n",
    "![Effective Approaches to Attention-based Neural Machine Translation](https://render.githubusercontent.com/render/math?math=score%28h_t%2C%20%5Cbar%20h_s%29%20%3D%0A%5Cbegin%7Bcases%7D%0Ah_t%20%5E%5Ctop%20%5Cbar%20h_s%20%26amp%3B%20dot%20%5C%5C%0Ah_t%20%5E%5Ctop%20%5Ctextbf%7BW%7D_a%20%5Cbar%20h_s%20%26amp%3B%20general%20%5C%5C%0Av_a%20%5E%5Ctop%20%5Ctextbf%7BW%7D_a%20%5B%20h_t%20%3B%20%5Cbar%20h_s%20%5D%20%26amp%3B%20concat%0A%5Cend%7Bcases%7D&mode=display)\n",
    "\n",
    "The modular definition of the scripting functions gives us an opportunity to build specific attention module that can switch between the different score methods. The input to this module is always the hidden state (of the decoder RNN) and set of encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 32\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_len=MAX_SENT_LENGTH):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        # define hyperparameters\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # define architecture depending on the method\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len))  # B x 1 x S\n",
    "\n",
    "        # calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        attn_energies = attn_energies.view(-1)\n",
    "        return F.softmax(attn_energies, dim=0).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "\n",
    "        return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a decoder that plugs this `Attention` module in after RNN to calculate attention weights, and apply those weights to the encoder outputs to get a context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "\n",
    "        # define network hyperparameters\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # define network arhcitecture\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size,\n",
    "                          n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "        # choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attention(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # note: we run this one step at a time\n",
    "        # get the embedding of the current input woed (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1)  # S=1xBxN\n",
    "\n",
    "        # combine embedded input word and last context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # Bx1xN\n",
    "\n",
    "        # final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0)  # S=1XbXN -> BxN\n",
    "        context = context.squeeze(1)  # BxS=1xN -> BxN\n",
    "        ctx_out_cat = torch.cat((rnn_output, context), 1)\n",
    "        output = F.log_softmax(self.out(ctx_out_cat), dim=1)\n",
    "\n",
    "        # return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing models\n",
    "\n",
    "To make sure the `Encoder` and `Decoder` model are working (and weights together) we'll do a quick test with fake word inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(10, 50)\n",
      "  (gru): GRU(50, 50, num_layers=2)\n",
      ")\n",
      "AttentionDecoderRNN(\n",
      "  (embedding): Embedding(10, 50)\n",
      "  (gru): GRU(100, 50, num_layers=2, dropout=0.1)\n",
      "  (out): Linear(in_features=100, out_features=10)\n",
      "  (attn): Attention(\n",
      "    (attn): Linear(in_features=50, out_features=50)\n",
      "  )\n",
      ")\n",
      "\n",
      "Decoded output=> \n",
      "-2.2149 -2.3126 -2.1680 -2.2941 -2.2682 -2.3453 -2.2841 -2.4082 -2.2984 -2.4652\n",
      "[torch.FloatTensor of size 1x10]\n",
      " hidden state=> \n",
      "torch.Size([2, 1, 50]) \n",
      "attention=> \n",
      "(0 ,.,.) = \n",
      "  0.3438  0.3325  0.3237\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "\n",
      "Decoded output=> \n",
      "-2.1976 -2.2813 -2.0996 -2.3142 -2.2876 -2.3663 -2.3250 -2.4028 -2.3096 -2.4943\n",
      "[torch.FloatTensor of size 1x10]\n",
      " hidden state=> \n",
      "torch.Size([2, 1, 50]) \n",
      "attention=> \n",
      "(0 ,.,.) = \n",
      "  0.3476  0.3267  0.3257\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n",
      "\n",
      "Decoded output=> \n",
      "-2.1959 -2.3061 -2.1015 -2.3447 -2.2868 -2.3615 -2.2871 -2.3750 -2.3262 -2.4910\n",
      "[torch.FloatTensor of size 1x10]\n",
      " hidden state=> \n",
      "torch.Size([2, 1, 50]) \n",
      "attention=> \n",
      "(0 ,.,.) = \n",
      "  0.3483  0.3260  0.3257\n",
      "[torch.FloatTensor of size 1x1x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_pair = random.choice(pairs)\n",
    "training_pair = variables_from_pair(input_lang, output_lang, random_pair)\n",
    "\n",
    "t_n_layers = 2\n",
    "t_hidden_size = 50\n",
    "\n",
    "encoder_test = EncoderRNN(10, t_hidden_size, t_n_layers)\n",
    "decoder_test = AttentionDecoderRNN('general', t_hidden_size, 10, t_n_layers)\n",
    "\n",
    "print(encoder_test)\n",
    "print(decoder_test)\n",
    "\n",
    "encoder_hidden = encoder_test.init_hidden()\n",
    "word_input = Variable(torch.LongTensor([1, 2, 3]))\n",
    "# word_input = training_pair[0]\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "\n",
    "word_inputs = Variable(torch.LongTensor([1, 2, 3]))\n",
    "decoder_attns = torch.zeros(1, 3, 3)\n",
    "decoder_hidden = encoder_hidden\n",
    "decoder_context = Variable(torch.zeros(1, decoder_test.hidden_size))\n",
    "\n",
    "for i in range(3):\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder_test(\n",
    "        word_inputs[i], decoder_context, decoder_hidden, encoder_outputs)\n",
    "    print('\\nDecoded output=> {} hidden state=> \\n{} \\nattention=> {}'.format(\n",
    "        decoder_output.data, decoder_hidden.size(), decoder_attn.data))\n",
    "    decoder_attns[0, i] = decoder_attn.squeeze(0).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train we first run the input sentence through the encoder word by word, and keep track of every output and the latest hidden state. Next the decoder is given the last hidden state of the encoder as its first hidden state, and the <SOS> token as its first input.\n",
    "From there we iterate to predict a next token from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 0.5\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_sent_len=MAX_SENT_LENGTH):\n",
    "\n",
    "    # zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0  # added onto for each word\n",
    "\n",
    "    # get size of input and target sentences\n",
    "    input_len = input_variable.size()[0]\n",
    "    target_len = target_variable.size()[0]\n",
    "\n",
    "    # run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # prepare input and output variables for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    # use last hidden state from encoder to start decoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # teacher forcing: use the ground-truth target as the next input\n",
    "        for di in range(target_len):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # next target is next input\n",
    "    else:\n",
    "        # withount teacher forcing: use network own prediction as the next input\n",
    "        for di in range(target_len):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "\n",
    "            # get most likely word index (highest value) from output\n",
    "            top_value, top_index = decoder_output.data.topk(1)\n",
    "            ni = top_index[0][0]\n",
    "\n",
    "            # chosen word is next input\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally helper functions to print time elapsed and estimated time remaining, given the current time and progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training\n",
    "\n",
    "With everything in place we can actually initialize a network and start training.\n",
    "To start we initialize models, optimizers, and a loss function (criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = 'general'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "# initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = AttentionDecoderRNN(\n",
    "    attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# movinitialize optimizers and criterion\n",
    "lr = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then setup variables for plotting and tracking progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuring training\n",
    "n_epochs = 20\n",
    "n_iterations = len(pairs)\n",
    "plot_every = 1000\n",
    "print_every = 500\n",
    "save_model_every = 1\n",
    "\n",
    "# keep track of time elapsed and running averages\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # reset every print_every\n",
    "plot_loss_total = 0  # reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually train, we call the `train` function many times, printing a summary as we go.\n",
    "\n",
    "*Note*: If you run this notebook you can train, interrupt the kernel, evaluate, and continue training later. You can comment out the lines above where the encoder and decoder are initialized (so they aren't reset) or simply run the notebook starting from the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is about to start training...\n",
      "0m 1s (- 27146m 14s) (e: 1 1 0%) 0.0198\n",
      "10m 53s (- 23740m 11s) (e: 501 1 0%) 4.9986\n",
      "22m 41s (- 24751m 39s) (e: 1001 1 0%) 4.7145\n",
      "35m 13s (- 25604m 30s) (e: 1501 1 0%) 4.6050\n",
      "47m 46s (- 26038m 13s) (e: 2001 1 0%) 4.7562\n",
      "60m 24s (- 26333m 5s) (e: 2501 1 0%) 4.7161\n",
      "73m 4s (- 26536m 38s) (e: 3001 1 0%) 4.3892\n",
      "85m 42s (- 26665m 55s) (e: 3501 1 0%) 4.4740\n",
      "98m 33s (- 26816m 13s) (e: 4001 1 0%) 4.3423\n",
      "111m 25s (- 26937m 59s) (e: 4501 1 0%) 4.6449\n",
      "122m 51s (- 26720m 7s) (e: 5001 1 0%) 4.3042\n",
      "133m 48s (- 26445m 19s) (e: 5501 1 0%) 4.3957\n",
      "144m 56s (- 26246m 45s) (e: 6001 1 0%) 4.2375\n",
      "156m 12s (- 26100m 11s) (e: 6501 1 0%) 4.3387\n",
      "167m 37s (- 25994m 24s) (e: 7001 1 0%) 4.3639\n",
      "179m 22s (- 25949m 48s) (e: 7501 1 0%) 4.4329\n",
      "190m 48s (- 25867m 18s) (e: 8001 1 0%) 4.1573\n",
      "202m 36s (- 25840m 12s) (e: 8501 1 0%) 4.4647\n",
      "215m 14s (- 25914m 46s) (e: 9001 1 0%) 4.3177\n",
      "227m 21s (- 25920m 48s) (e: 9501 1 0%) 4.3925\n",
      "239m 28s (- 25925m 52s) (e: 10001 1 0%) 4.3654\n",
      "252m 4s (- 25978m 22s) (e: 10501 1 0%) 4.3851\n",
      "264m 44s (- 26030m 48s) (e: 11001 1 0%) 4.2878\n",
      "277m 16s (- 26066m 26s) (e: 11501 1 0%) 4.2446\n",
      "290m 7s (- 26125m 51s) (e: 12001 1 0%) 4.4960\n",
      "303m 27s (- 26220m 52s) (e: 12501 1 0%) 4.2943\n",
      "316m 31s (- 26286m 24s) (e: 13001 1 0%) 4.2533\n",
      "329m 27s (- 26335m 45s) (e: 13501 1 0%) 4.0234\n",
      "342m 40s (- 26400m 42s) (e: 14001 1 0%) 4.4402\n",
      "356m 34s (- 26512m 17s) (e: 14501 1 0%) 4.1944\n",
      "369m 52s (- 26572m 13s) (e: 15001 1 0%) 4.4418\n",
      "383m 18s (- 26637m 20s) (e: 15501 1 0%) 3.9165\n",
      "396m 57s (- 26710m 57s) (e: 16001 1 0%) 4.4088\n",
      "411m 14s (- 26820m 54s) (e: 16501 1 0%) 3.8053\n",
      "424m 43s (- 26873m 53s) (e: 17001 1 0%) 4.2752\n",
      "438m 34s (- 26944m 53s) (e: 17501 1 0%) 3.8241\n",
      "452m 52s (- 27037m 54s) (e: 18001 1 0%) 4.1752\n",
      "467m 32s (- 27146m 14s) (e: 18501 1 0%) 4.0281\n",
      "482m 2s (- 27238m 53s) (e: 19001 1 0%) 4.2162\n",
      "496m 20s (- 27314m 51s) (e: 19501 1 0%) 3.8046\n",
      "510m 54s (- 27401m 2s) (e: 20001 1 0%) 4.3506\n",
      "526m 17s (- 27524m 33s) (e: 20501 1 0%) 4.1862\n",
      "540m 47s (- 27597m 10s) (e: 21001 1 0%) 3.9033\n",
      "555m 57s (- 27698m 16s) (e: 21501 1 0%) 3.8199\n",
      "570m 50s (- 27780m 30s) (e: 22001 1 0%) 4.2756\n",
      "586m 3s (- 27874m 15s) (e: 22501 1 0%) 3.8330\n",
      "601m 20s (- 27966m 3s) (e: 23001 1 0%) 4.4451\n",
      "616m 13s (- 28036m 0s) (e: 23501 1 0%) 3.6583\n",
      "631m 5s (- 28100m 29s) (e: 24001 1 0%) 4.1614\n",
      "646m 36s (- 28190m 51s) (e: 24501 1 0%) 4.0563\n",
      "662m 2s (- 28273m 2s) (e: 25001 1 0%) 4.4210\n",
      "677m 17s (- 28344m 25s) (e: 25501 1 0%) 3.2765\n",
      "692m 33s (- 28412m 23s) (e: 26001 1 0%) 3.7985\n",
      "709m 23s (- 28540m 16s) (e: 26501 1 0%) 4.0549\n",
      "727m 27s (- 28712m 11s) (e: 27001 1 0%) 3.8006\n",
      "742m 37s (- 28764m 19s) (e: 27501 1 0%) 4.7017\n",
      "758m 13s (- 28830m 8s) (e: 28001 1 0%) 3.8098\n",
      "774m 5s (- 28903m 56s) (e: 28501 1 0%) 3.8312\n",
      "790m 32s (- 28995m 16s) (e: 29001 1 0%) 3.8066\n",
      "806m 4s (- 29050m 16s) (e: 29501 1 0%) 4.7109\n",
      "821m 52s (- 29112m 35s) (e: 30001 1 0%) 3.7116\n",
      "838m 19s (- 29194m 56s) (e: 30501 1 0%) 3.8217\n",
      "855m 27s (- 29297m 7s) (e: 31001 1 0%) 4.0241\n",
      "871m 11s (- 29348m 22s) (e: 31501 1 0%) 4.5406\n",
      "887m 30s (- 29417m 14s) (e: 32001 1 0%) 3.6955\n",
      "904m 1s (- 29489m 57s) (e: 32501 1 0%) 4.1509\n",
      "921m 9s (- 29579m 37s) (e: 33001 1 0%) 3.8155\n",
      "937m 21s (- 29636m 33s) (e: 33501 1 0%) 4.7230\n",
      "954m 4s (- 29707m 4s) (e: 34001 1 0%) 3.8478\n",
      "971m 43s (- 29804m 9s) (e: 34501 1 0%) 4.0924\n",
      "990m 8s (- 29921m 5s) (e: 35001 1 0%) 3.8477\n",
      "1007m 26s (- 30000m 59s) (e: 35501 1 0%) 4.6632\n",
      "1027m 49s (- 30168m 32s) (e: 36001 1 0%) 3.8080\n",
      "1045m 31s (- 30253m 23s) (e: 36501 1 0%) 3.9870\n",
      "1063m 10s (- 30334m 20s) (e: 37001 1 0%) 3.9706\n",
      "1080m 34s (- 30405m 1s) (e: 37501 1 0%) 4.3861\n",
      "1099m 5s (- 30504m 32s) (e: 38001 1 0%) 3.9506\n",
      "1117m 57s (- 30611m 4s) (e: 38501 1 0%) 3.8696\n",
      "1135m 42s (- 30683m 47s) (e: 39001 1 0%) 4.7935\n",
      "1154m 10s (- 30773m 11s) (e: 39501 1 0%) 3.6176\n",
      "1172m 44s (- 30862m 41s) (e: 40001 1 0%) 3.9834\n",
      "1191m 8s (- 30945m 17s) (e: 40501 1 0%) 4.1483\n",
      "1209m 12s (- 31017m 2s) (e: 41001 1 0%) 4.0110\n",
      "1228m 10s (- 31108m 58s) (e: 41501 1 0%) 3.9659\n",
      "1247m 1s (- 31195m 26s) (e: 42001 1 0%) 4.3476\n",
      "1266m 1s (- 31283m 22s) (e: 42501 1 0%) 3.7892\n",
      "1285m 19s (- 31376m 0s) (e: 43001 1 0%) 3.9918\n",
      "1304m 25s (- 31461m 16s) (e: 43501 1 0%) 4.4442\n",
      "1323m 46s (- 31550m 22s) (e: 44001 1 0%) 3.5981\n",
      "1343m 22s (- 31642m 29s) (e: 44501 1 0%) 4.4149\n",
      "1362m 39s (- 31724m 54s) (e: 45001 1 0%) 3.8908\n",
      "1382m 25s (- 31816m 13s) (e: 45501 1 0%) 4.1282\n",
      "1402m 21s (- 31908m 56s) (e: 46001 1 0%) 4.0345\n",
      "1422m 15s (- 31998m 22s) (e: 46501 1 0%) 4.2635\n",
      "1442m 27s (- 32092m 33s) (e: 47001 1 0%) 3.9589\n",
      "1462m 31s (- 32180m 55s) (e: 47501 1 0%) 4.5701\n",
      "1483m 21s (- 32284m 5s) (e: 48001 1 0%) 3.8239\n",
      "1503m 34s (- 32371m 6s) (e: 48501 1 0%) 4.4793\n",
      "1523m 54s (- 32458m 40s) (e: 49001 1 0%) 4.3749\n",
      "1549m 58s (- 32664m 37s) (e: 49501 1 0%) 3.7618\n",
      "1570m 5s (- 32742m 0s) (e: 50001 1 0%) 4.1629\n",
      "1590m 27s (- 32822m 29s) (e: 50501 1 0%) 4.1451\n",
      "1611m 8s (- 32907m 36s) (e: 51001 1 0%) 3.9377\n",
      "1631m 57s (- 32993m 33s) (e: 51501 1 0%) 4.0829\n",
      "1653m 0s (- 33081m 43s) (e: 52001 1 0%) 3.9926\n",
      "1674m 10s (- 33170m 27s) (e: 52501 1 0%) 4.4722\n",
      "1695m 32s (- 33260m 35s) (e: 53001 1 0%) 4.7596\n",
      "1717m 41s (- 33364m 27s) (e: 53501 1 0%) 4.4799\n",
      "1740m 23s (- 33476m 6s) (e: 54001 1 0%) 4.6183\n",
      "1764m 19s (- 33608m 56s) (e: 54501 1 0%) 4.9891\n",
      "1771m 32s (- 33658m 34s) (e: 54636 2 0%) 1.5277\n",
      "1786m 7s (- 33611m 44s) (e: 55136 2 0%) 3.9490\n",
      "1801m 24s (- 33578m 29s) (e: 55636 2 0%) 3.2969\n",
      "1817m 2s (- 33552m 4s) (e: 56136 2 0%) 3.1929\n",
      "1832m 32s (- 33523m 20s) (e: 56636 2 0%) 3.5198\n",
      "1848m 34s (- 33504m 35s) (e: 57136 2 0%) 3.1261\n",
      "1864m 27s (- 33483m 4s) (e: 57636 2 0%) 3.2492\n",
      "1880m 15s (- 33460m 22s) (e: 58136 2 0%) 3.1343\n",
      "1896m 10s (- 33439m 39s) (e: 58636 2 0%) 2.9891\n",
      "1912m 7s (- 33419m 32s) (e: 59136 2 0%) 3.3732\n",
      "1928m 28s (- 33406m 29s) (e: 59636 2 0%) 2.9478\n",
      "1944m 31s (- 33388m 27s) (e: 60136 2 0%) 3.0451\n",
      "1961m 23s (- 33384m 9s) (e: 60636 2 0%) 2.9535\n",
      "1978m 23s (- 33381m 47s) (e: 61136 2 0%) 3.1861\n",
      "1995m 22s (- 33379m 10s) (e: 61636 2 0%) 3.0795\n",
      "2012m 49s (- 33383m 51s) (e: 62136 2 0%) 3.1264\n",
      "2029m 39s (- 33378m 10s) (e: 62636 2 0%) 3.1887\n",
      "2047m 38s (- 33391m 1s) (e: 63136 2 0%) 3.0105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-438fef08c313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# run the train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         loss = train(input_variable, target_variable, encoder, decoder,\n\u001b[0;32m---> 16\u001b[0;31m                      encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# keep track loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d4204ab9fbb1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_sent_len)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/handson.ai/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/handson.ai/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# let's begin\n",
    "start = time.time()\n",
    "print('Is about to start training...')\n",
    "trained_model_path = 'nmt-at-epoch-{}.pt'\n",
    "\n",
    "step = 1\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for it in range(n_iterations):\n",
    "        # get training data for this cycle\n",
    "        training_pair = variables_from_pair(input_lang, output_lang, pairs[it])\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "\n",
    "        # run the train function\n",
    "        loss = train(input_variable, target_variable, encoder, decoder,\n",
    "                     encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "        # keep track loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if it == 0:\n",
    "            continue\n",
    "\n",
    "        if it % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (e: %d %d %d%%) %.4f' % (time_since(\n",
    "                start, step / (n_epochs * n_iterations)), step, epoch, (step / (n_epochs * n_iterations)) * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "\n",
    "        if it % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_loss_total = 0\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "        step += 1\n",
    "\n",
    "    if epoch % save_model_every == 0:\n",
    "        torch.save({\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'decoder': decoder.state_dict(),\n",
    "            'encoder_optimizer': encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer': decoder_optimizer.state_dict()\n",
    "        },\n",
    "            trained_model_path.format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training loss\n",
    "\n",
    "Plotting is done with `matplotlib`, using the array `plot_loses` that was created while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def draw_loss_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)  # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "draw_loss_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the network\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets. Instead we always feed decoder' predictions back itself.\n",
    "Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_sent_len=MAX_SENT_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_len = input_variable.size()[0]\n",
    "\n",
    "    # run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_sent_len, max_sent_len)\n",
    "\n",
    "    # run through decoder\n",
    "    for di in range(max_sent_len):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_output, decoder_context, decoder_hidden, encoder_outputs)\n",
    "        decoder_attentions[di, :decoder_attention.size(\n",
    "            2)] += decoder_attention.squeeze(0).squeeze(0).data\n",
    "\n",
    "        top_val, top_idx = decoder_output.data.topk(1)\n",
    "        ni = top_idx[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word([[ni]]))\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "\n",
    "    output_words, decoder_attn = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "\n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "for i in range(len(a)):\n",
    "    print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
